{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4: Extracting syntactic groups using recurrent networks\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will create a system to extract syntactic groups from a text. You will apply it to the CoNLL 2000 dataset. You will train your models with PyTorch.\n",
    "\n",
    "Be aware that in PyTorch, the data matrices, by default, have an unconventional ordering with recurrent networks. To have a batch ordering similar to what we saw during the course, you must use the `batch_first=True` argument. See here https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html and https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "Before you start the assignment, please run the prerequisites from the prerequistites notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of this assignment are to:\n",
    "* Write a program to detect partial syntactic structures called groups or chunks\n",
    "* Understand the principles of supervised machine learning techniques applied to language processing\n",
    "* Write a short report of 2 to 3 pages on the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This instruction may solve installation conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import conlleval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeds\n",
    "Making things reproduceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1dadf34d410>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "LSTM_HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to adjust the paths to load the datasets from your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'corpus/train.txt'\n",
    "test_file = 'corpus/test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now convert the dataset in a Python data structure. Read the functions below to load the datasets. They store the corpus in a list of sentences. Each sentence is a list of rows, where each row is a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file):\n",
    "    \"\"\"\n",
    "    Creates a list of sentences from the corpus\n",
    "    Each sentence is a string\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f = open(file).read().strip()\n",
    "    sentences = f.split('\\n\\n')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rows(sentences, column_names):\n",
    "    \"\"\"\n",
    "    Creates a list of sentence where each sentence is a list of lines\n",
    "    Each line is a dictionary of columns\n",
    "    :param sentences:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        rows = sentence.split('\\n')\n",
    "        sentence = [dict(zip(column_names, row.split())) for row in rows]\n",
    "        new_sentences.append(sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CoNLL 2000 files have three columns: The wordform, `form`, its part of speech, `pos`, and the tag denoting the syntactic group also called the chunk tag, `chunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['form', 'pos', 'chunk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the corpus as a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'He', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'reckons', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'current', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'account', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'will', 'pos': 'MD', 'chunk': 'B-VP'},\n",
       "  {'form': 'narrow', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'only', 'pos': 'RB', 'chunk': 'B-NP'},\n",
       "  {'form': '#', 'pos': '#', 'chunk': 'I-NP'},\n",
       "  {'form': '1.8', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'billion', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_dict = split_rows(train_sentences, column_names)\n",
    "train_dict[10:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = 'corpus/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function below that reads GloVe embeddings and store them in a dictionary, where the keys will be the words and the values, the embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    glove = open(file, encoding='utf8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Git-Repos\\Language-Technology\\labs_2023\\Lab_5\\5-chunker.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We read the embeddings\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embeddings_dict \u001b[39m=\u001b[39m read_embeddings(embedding_file)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m embedded_words \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\u001b[39mlist\u001b[39m(embeddings_dict\u001b[39m.\u001b[39mkeys()))\n",
      "\u001b[1;32mc:\\Git-Repos\\Language-Technology\\labs_2023\\Lab_5\\5-chunker.ipynb Cell 31\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m embeddings \u001b[39m=\u001b[39m {}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m glove \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(file, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m glove:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     values \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     word \u001b[39m=\u001b[39m values[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We read the embeddings\n",
    "embeddings_dict = read_embeddings(embedding_file)\n",
    "embedded_words = sorted(list(embeddings_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# words in embedding dictionary: 400000'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'# words in embedding dictionary: {}'.format(len(embedded_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chording',\n",
       " 'chordoma',\n",
       " 'chordophones',\n",
       " 'chords',\n",
       " 'chore',\n",
       " 'chorea',\n",
       " 'chorene',\n",
       " 'choreograph',\n",
       " 'choreographed',\n",
       " 'choreographer']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_words[100000:100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.51973,  1.0395 ,  0.20924,  0.16285,  0.7209 ,  0.81524,\n",
       "       -0.34641, -0.76654, -0.49576,  0.24634,  0.44094,  0.37701,\n",
       "       -0.16396,  0.2775 ,  0.16563,  0.43869, -1.0887 ,  0.12663,\n",
       "        0.66916,  0.3578 ], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['chords'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a cosine similarity, write a `closest(target_word, embeddings, count=10)` that computes the 10 closest words to the words _table_, _france_, and _sweden_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(array1, array2):\n",
    "        dot_product = np.dot(array1, array2)\n",
    "        cosine_similarity = dot_product/(np.linalg.norm(array1)*np.linalg.norm(array2))\n",
    "        return cosine_similarity\n",
    "\n",
    "def closest2(target_word, embeddings, count=10):\n",
    "    candidates = []\n",
    "    target_embedding = np.array(embeddings[target_word])\n",
    "    \n",
    "    for word in embeddings:\n",
    "        similarity = cosine_similarity(target_embedding, np.array(embeddings[word]))\n",
    "        candidates.append((word, similarity))    \n",
    "    \n",
    "    closest = sorted(candidates, key=lambda a: a[1], reverse=True)\n",
    "    closest_words = [x[0] for x in closest[0:count]]\n",
    "    return closest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['table',\n",
       " 'tables',\n",
       " 'place',\n",
       " 'bottom',\n",
       " 'room',\n",
       " 'side',\n",
       " 'sit',\n",
       " 'top',\n",
       " 'here',\n",
       " 'pool']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest2('table', embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['france',\n",
       " 'belgium',\n",
       " 'french',\n",
       " 'britain',\n",
       " 'spain',\n",
       " 'paris',\n",
       " 'germany',\n",
       " 'italy',\n",
       " 'europe',\n",
       " 'netherlands']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest2('france', embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sweden',\n",
       " 'denmark',\n",
       " 'norway',\n",
       " 'finland',\n",
       " 'netherlands',\n",
       " 'austria',\n",
       " 'switzerland',\n",
       " 'germany',\n",
       " 'swedish',\n",
       " 'belgium']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest2('sweden', embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the ${X}$ and ${Y}$ Lists of Symbols from the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence, you will build an input sequence, $\\mathbf{x}$, corresponding to the words and an output one, $\\mathbf{y}$, corresponding to the chunk tags.\n",
    "\n",
    "Write a `build_sequences(corpus_dict, key_x='form', key_y='chunk', tolower=True)` function that, for each sentence, returns the $\\mathbf{x}$ and $\\mathbf{y}$ lists of symbols consisting of words and chunk tags. Set the words in lower case if `tolower` is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 11th sentence of the training set, you should have:<br/>\n",
    "`x = ['he',  'reckons',  'the',  'current',  'account',  'deficit',  'will',  'narrow',  'to',  'only',  '#',  '1.8',  'billion',  'in',  'september',  '.']`\n",
    "\n",
    "`y = ['B-NP', 'B-VP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-VP', 'I-VP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-PP', 'B-NP', 'O']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def build_sequences(corpus_dict, key_x='form', key_y='pos', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        if tolower:\n",
    "            keys_x = [x[key_x].lower() for x in sentence] \n",
    "        else:\n",
    "            keys_x = [x[key_x] for x in sentence] \n",
    "        keys_y = [y[key_y] for y in sentence]\n",
    "        \n",
    "        X.append(keys_x)\n",
    "        Y.append(keys_y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_symbs, Y_train_symbs = build_sequences(train_dict, key_x='form', key_y='chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'reckons', 'the', 'current', 'account', 'deficit', 'will', 'narrow', 'to', 'only', '#', '1.8', 'billion', 'in', 'september', '.']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_symbs[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-NP', 'B-VP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-VP', 'I-VP', 'B-PP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-PP', 'B-NP', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_symbs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vocabulary of all the words observed in the training set as well as in GloVe. You should find 401,464 different words. You will proceed in two steps.\n",
    "\n",
    "First extract the list of unique words `words` from the CoNLL training set and the list of chunk tags, `chunks`. You will sort them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_words = []\n",
    "for sentence in X_train_symbs:\n",
    "    training_words += sentence\n",
    "training_words = sorted(set(training_words))\n",
    "\n",
    "chunks = []\n",
    "for sentence in Y_train_symbs:\n",
    "    chunks += sentence\n",
    "chunks = sorted(set(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words seen in training corpus: 17258\n",
      "# Chunks tags seen: 22\n"
     ]
    }
   ],
   "source": [
    "print('# words seen in training corpus:', len(training_words))\n",
    "print('# Chunks tags seen:', len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['casinos',\n",
       " 'caspita',\n",
       " 'caspita-brand',\n",
       " 'cassettes',\n",
       " 'cast',\n",
       " 'castigated',\n",
       " 'castigating',\n",
       " 'castillo',\n",
       " 'casting',\n",
       " 'castro-medellin']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_words[4000:4010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ADJP',\n",
       " 'B-ADVP',\n",
       " 'B-CONJP',\n",
       " 'B-INTJ',\n",
       " 'B-LST',\n",
       " 'B-NP',\n",
       " 'B-PP',\n",
       " 'B-PRT',\n",
       " 'B-SBAR',\n",
       " 'B-UCP']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, merge the list of unique CoNLL words with the words in the embeddings file. You will sort this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code: Add vocabulary of embedded words\n",
    "vocabulary_words = sorted(set(embedded_words + training_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in the vocabulary: embeddings and corpus: 401464\n"
     ]
    }
   ],
   "source": [
    "print('# words in the vocabulary: embeddings and corpus:', len(vocabulary_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joy',\n",
       " 'joya',\n",
       " 'joyal',\n",
       " 'joyandet',\n",
       " 'joyas',\n",
       " 'joyce',\n",
       " 'joycean',\n",
       " 'joycelyn',\n",
       " 'joyces',\n",
       " 'joydeep']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_words[200000:200010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the indices `word2idx`, `chunk2idx` and inverted indices `idx2word`, `idx2chunk` for the words and the chunk tags: i.e. you will associate each word with a number. You will use index 0 for the padding symbol and 1 for unknown words. This means that your first word will start at index 2. For the chunks, you will start at index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {}\n",
    "word2idx = {}\n",
    "for i, word in enumerate(vocabulary_words):\n",
    "    idx = i + 2\n",
    "    idx2word[idx] = word\n",
    "    word2idx[word] = idx\n",
    "\n",
    "idx2chunk = {}\n",
    "chunk2idx = {}\n",
    "for i, chunk in enumerate(chunks):\n",
    "    idx = i + 1\n",
    "    idx2chunk[idx] = chunk\n",
    "    chunk2idx[chunk] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 2), ('!!', 3), ('!!!', 4), ('!!!!', 5), ('!!!!!', 6), ('!?', 7), ('!?!', 8), ('\"', 9), ('#', 10), ('##', 11), ('###', 12), ('#a', 13), ('#aabccc', 14), ('#b', 15), ('#c', 16), ('#cc', 17), ('#ccc', 18), ('#cccccc', 19), ('#ccccff', 20), ('#d', 21), ('#daa', 22), ('#dcdcdc', 23), ('#e', 24), ('#f', 25), ('#faf', 26)]\n"
     ]
    }
   ],
   "source": [
    "print(list(word2idx.items())[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunk indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-ADJP': 1, 'B-ADVP': 2, 'B-CONJP': 3, 'B-INTJ': 4, 'B-LST': 5, 'B-NP': 6, 'B-PP': 7, 'B-PRT': 8, 'B-SBAR': 9, 'B-UCP': 10, 'B-VP': 11, 'I-ADJP': 12, 'I-ADVP': 13, 'I-CONJP': 14, 'I-INTJ': 15, 'I-NP': 16, 'I-PP': 17, 'I-PRT': 18, 'I-SBAR': 19, 'I-UCP': 20, 'I-VP': 21, 'O': 22}\n"
     ]
    }
   ],
   "source": [
    "print(chunk2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a numpy matrix of dimensions $(M, N)$, where $M$ will be the size of the vocabulary: The unique words in the training set and the words in GloVe, and $N$, the dimension of the embeddings.\n",
    "The padding symbol and the unknown word symbol will be part of the vocabulary at respectively index 0 and 1. \n",
    "\n",
    "Initialize the matrix with random values with the `np.random.uniform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add two dimensions for the padding symbol at index 0 and unknown words at index 1\n",
    "embedding_matrix = np.random.uniform(-0.05, 0.05, (len(vocabulary_words) + 2, EMBEDDING_DIM))\n",
    "# embedding_matrix = np.random.random((len(vocabulary_words) + 2, EMBEDDING_DIM))\n",
    "# embedding_matrix = np.zeros((len(vocabulary_words) + 2, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of your matrix is: (401466, 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(401466, 100)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the matrix with the GloVe embeddings when available. This means: Replace the random vector with an embedding when available. You will use the indices from the previous section. You will call `out_of_embeddings` the list of words in CoNLL, but not in the embedding list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "out_of_embeddings = []\n",
    "for word in vocabulary_words:\n",
    "    if word in embeddings_dict:\n",
    "        embedding = embeddings_dict[word]\n",
    "        index = word2idx[word]\n",
    "        embedding_matrix[index] = embedding\n",
    "    else: \n",
    "        out_of_embeddings.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1464"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_of_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"y'all\",\n",
       " 'yankus',\n",
       " 'year-ago',\n",
       " 'year-before',\n",
       " 'year-earlier',\n",
       " 'year-to-date',\n",
       " 'yield-management',\n",
       " 'zaishuo',\n",
       " 'zarett',\n",
       " 'zumbrunn']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_embeddings[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings of the padding symbol, idx 0, random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03084805,  0.01221088, -0.00622723,  0.02853586,  0.02799758,\n",
       "       -0.02274074, -0.02235357,  0.03018722,  0.04581394,  0.03759326])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings of the word _table_, the GloVe values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.61453998,  0.89692998,  0.56770998,  0.39102   , -0.22437   ,\n",
       "        0.49035001,  0.10868   ,  0.27410999, -0.23833001, -0.52152997])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[word2idx['table']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings of _zarett_, a word in CoNLL 2000, but not in GloVe, random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04485961, -0.01950363,  0.03356147, -0.02404349, -0.04000838,\n",
       "        0.01959841, -0.03943566, -0.01355046,  0.00896135, -0.02441297])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[word2idx['zarett']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the ${X}$ and ${Y}$ Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create the input and output sequences with numerical indices. First, convert the \n",
    "${X}_\\text{train\\_symbs}$ and ${Y}_\\text{train\\_symbs}$ \n",
    "lists of symbols in lists of numbers using the indices you created. Call them `X_train_idx` and `Y_train_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "# We create the parallel sequences of indexes\n",
    "X_train_idx = []\n",
    "Y_train_idx = []\n",
    "for x, y in zip(X_train_symbs, Y_train_symbs):\n",
    "    X_train_idx.append([word2idx[word] for word in x])\n",
    "    Y_train_idx.append([chunk2idx[chunk] for chunk in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word indices of the three first sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107701, 189360, 358640, 291209, 193879, 388606, 143496, 362305, 353285, 56501, 328878, 126632, 187522, 364843, 148777, 152124, 326524, 454, 131007, 152124, 306232, 363097, 454, 144953, 362305, 331257, 43426, 347508, 189267, 155109, 200552, 55175, 63614, 154, 259236, 120001, 873], [97171, 269136, 358640, 143112, 262191, 219534, 154, 307829, 106548, 362305, 43426, 149626, 249511, 288933, 174855, 177388, 362305, 293204, 43426, 154301, 189360, 344283, 274536, 358640, 279589, 386150, 873], [88319, 54890, 304156, 372747, 349558, 152124, 344283, 174855, 72318, 139858, 88675, 358640, 97171, 154, 144970, 362305, 56361, 57639, 261034, 288933, 240241, 189360, 180283, 234487, 183252, 340448, 218722, 360423, 873]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_idx[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk tag indices of the three first sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 7, 6, 16, 11, 21, 21, 21, 21, 6, 16, 16, 9, 6, 16, 7, 6, 22, 1, 7, 6, 6, 22, 11, 21, 21, 6, 16, 16, 7, 6, 16, 16, 6, 16, 16, 22], [22, 7, 6, 16, 6, 16, 6, 16, 16, 7, 6, 16, 16, 16, 11, 21, 21, 21, 6, 16, 7, 6, 7, 6, 16, 16, 22], [22, 6, 11, 6, 16, 7, 6, 11, 21, 21, 7, 6, 16, 6, 16, 11, 21, 6, 16, 16, 16, 7, 6, 16, 16, 16, 6, 16, 22]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_idx[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, pad the sentences using the `pad_sequences` function. After padding, the second sentence you look like (the indices are not necessarily the same).\n",
    "```\n",
    "x = [ 97171, 269136, 358640, 143112, 262191, 219534,    154, 307829, 106548,\n",
    "        362305,  43426, 149626, 249511, 288933, 174855, 177388, 362305, 293204,\n",
    "         43426, 154301, 189360, 344283, 274536, 358640, 279589, 386150,    873,\n",
    "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
    "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
    "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
    "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
    "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
    "             0,      0,      0,      0,      0,      0]\n",
    "y = [22,  7,  6, 16,  6, 16,  6, 16, 16,  7,  6, 16, 16, 16, 11, 21, 21, 21,\n",
    "         6, 16,  7,  6,  7,  6, 16, 16, 22,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "         0,  0,  0,  0,  0,  0]\n",
    "```\n",
    "\n",
    "You will call the results `X_train_padded` and `Y_train_padded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_idx = list(map(torch.LongTensor, X_train_idx))\n",
    "Y_train_idx = list(map(torch.LongTensor, Y_train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "X_train_padded = pad_sequence(X_train_idx, batch_first=True)\n",
    "Y_train_padded = pad_sequence(Y_train_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 97171, 269136, 358640, 143112, 262191, 219534,    154, 307829, 106548,\n",
       "        362305,  43426, 149626, 249511, 288933, 174855, 177388, 362305, 293204,\n",
       "         43426, 154301, 189360, 344283, 274536, 358640, 279589, 386150,    873,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22,  7,  6, 16,  6, 16,  6, 16, 16,  7,  6, 16, 16, 16, 11, 21, 21, 21,\n",
       "         6, 16,  7,  6,  7,  6, 16, 16, 22,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_padded[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your network consisting of one embedding layer, a simple recurrent neural network, either RNN or LSTM, and a linear layer. You will initialize the embedding layer with `embedding_matrix` using `from_pretrained()`. You may try other configurations after. As number of RNN/LSTM units use 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix, embedding_dim, lstm_units, nbr_classes, bidi_lstm=False):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            padding_idx=0)\n",
    "        if bidi_lstm:\n",
    "            self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True, bidirectional=True)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_dim, lstm_units, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(in_features=lstm_units*2, out_features=nbr_classes, bias=True)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = torch.relu(lstm_out)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(embedding_matrix, EMBEDDING_DIM, LSTM_HIDDEN_DIM, len(chunks) + 1, bidi_lstm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embeddings): Embedding(401466, 100, padding_idx=0)\n",
       "  (lstm): LSTM(100, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=23, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the loss `loss_fn` and optimizer `optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)    # cross entropy loss\n",
    "optimizer = torch.optim.RMSprop(model1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.LongTensor(X_train_padded)\n",
    "Y_train = torch.LongTensor(Y_train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X_train, Y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Few Experiments\n",
    "Flattening the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8936, 78])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 7, 6,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([697008])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.view(-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred = model1(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8936, 78, 23])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_pred.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8936, 78, 23])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_pred.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([697008, 23])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_pred.view(-1, Y_train_pred.size()[-1]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary to store the accuracy and the loss. Th exact values are difficult to compute because of the padding symbols. We include the padding symbols in the computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "history['accuracy'] = []\n",
    "history['loss'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [00:20<00:00, 13.98it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\b\\abs_abjetg6_iu\\croot\\pytorch_1686932924616\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6889156608 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Git-Repos\\Language-Technology\\labs_2023\\Lab_5\\5-chunker.ipynb Cell 115\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#Y220sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#Y220sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#Y220sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_accuracy \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39margmax(model1(X_train), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m Y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#Y220sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m history[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [train_accuracy\u001b[39m/\u001b[39mtorch\u001b[39m.\u001b[39mnumel(Y_train)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#Y220sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m history[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [train_loss\u001b[39m/\u001b[39mbatch_cnt]\n",
      "File \u001b[1;32mc:\\Users\\theo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Git-Repos\\Language-Technology\\labs_2023\\Lab_5\\5-chunker.ipynb Cell 115\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#Y220sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#Y220sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(sentence)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#Y220sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     lstm_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(embeds)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#Y220sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     lstm_out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(lstm_out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Git-Repos/Language-Technology/labs_2023/Lab_5/5-chunker.ipynb#Y220sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(lstm_out)\n",
      "File \u001b[1;32mc:\\Users\\theo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\theo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[0;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\b\\abs_abjetg6_iu\\croot\\pytorch_1686932924616\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6889156608 bytes."
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    word_cnt = 0\n",
    "    batch_cnt = 0\n",
    "    for X_batch, Y_batch in tqdm(dataloader):\n",
    "        batch_cnt += 1\n",
    "        Y_batch_pred = model1(X_batch)\n",
    "        loss = loss_fn(Y_batch_pred.view(-1, Y_batch_pred.shape[-1]), Y_batch.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_accuracy += torch.sum(torch.argmax(model1(X_train), dim=-1) == Y_train)\n",
    "    history['accuracy'] += [train_accuracy/torch.numel(Y_train)]\n",
    "    history['loss'] += [train_loss/batch_cnt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we visualize the training curves. Ideally, we would compare them with a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGxCAYAAABvIsx7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA01klEQVR4nO3deXiU1f3+8XvISiAZkEBCJEAQC6SIlVACsSFoNSyiUOlVBIlsRShVZPHHIioUKFtR0bJViqDFslTA0hYokU1awlpAChG3ICAZWYQkbFnP7w/KfB0SQgIMSQ7v13XNpXPmc57nnEN0bp4tDmOMEQAAgEUqlfUAAAAAbjUCDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOUA45HI4SvTZt2nRT+xk3bpwcDscN9d20adMtGQNunMPh0Lhx48p6GEC55OBXNQDlz7Zt2zzeT5gwQRs3btSGDRs82qOjoxUSEnLD+zl27JiOHTumVq1albpvZmamDh48eNNjwI3btm2b6tSpozp16pT1UIByh4ADVAC9e/fWBx98oHPnzhVbd+HCBQUFBd2mUaGkLl68qMDAwBs+Wgag9DhFBVRQbdu2VdOmTfXxxx8rLi5OQUFB6tu3ryRp6dKlSkxMVO3atVW5cmU1adJEo0aN0vnz5z22UdQpqvr166tTp05au3atmjdvrsqVK6tx48Z65513POqKOkXVu3dvVa1aVV988YU6duyoqlWrKjIyUsOHD1d2drZH/2PHjunnP/+5goODVa1aNT399NPauXOnHA6HFi5cWOzcT548qUGDBik6OlpVq1ZVrVq19PDDD2vLli2FarOzszV+/Hg1adJEgYGBqlGjhh566CFt3brVXVNQUKDf//73+tGPfqTKlSurWrVqatWqlVatWuWuudbpoPr166t3797u9wsXLpTD4dC6devUt29f1axZU0FBQcrOztYXX3yhPn366N5771VQUJDuvvtuPf7449q/f3+h7Z49e1bDhw9XgwYNFBAQoFq1aqljx4769NNPix2Ty+XSgAEDVKdOHfn7+ysqKkq/+c1vlJeX51E3Z84c3X///apataqCg4PVuHFjvfTSS8WuO1CR+Jb1AADcuPT0dPXs2VMjRozQpEmTVKnS5b+zfP755+rYsaOGDBmiKlWq6NNPP9XUqVO1Y8eOQqe5irJv3z4NHz5co0aNUlhYmP74xz+qX79+atiwodq0aVNs39zcXD3xxBPq16+fhg8fro8//lgTJkyQ0+nUq6++Kkk6f/68HnroIX333XeaOnWqGjZsqLVr16pbt24lmvd3330nSRo7dqzCw8N17tw5rVy5Um3bttX69evVtm1bSVJeXp46dOigLVu2aMiQIXr44YeVl5enbdu26ciRI4qLi5N0OZgtWrRI/fr10/jx4+Xv76///Oc/Onz4cInGU5S+ffvqscce05/+9CedP39efn5+On78uGrUqKEpU6aoZs2a+u677/Tuu+8qNjZWe/bsUaNGjSRJWVlZ+slPfqLDhw9r5MiRio2N1blz5/Txxx8rPT1djRs3LnKfLpdLLVu2VKVKlfTqq6/qnnvuUUpKiiZOnKjDhw9rwYIFkqQlS5Zo0KBBev755zV9+nRVqlRJX3zxhQ4ePHjD8wXKHQOg3OvVq5epUqWKR1tCQoKRZNavX19s34KCApObm2s2b95sJJl9+/a5Pxs7dqy5+n8D9erVM4GBgebrr792t128eNHcddddZsCAAe62jRs3Gklm48aNHuOUZJYtW+axzY4dO5pGjRq538+aNctIMmvWrPGoGzBggJFkFixYUOycrpaXl2dyc3PNT3/6U/Ozn/3M3f7ee+8ZSWbevHnX7Pvxxx8bSWbMmDHF7kOSGTt2bKH2evXqmV69ernfL1iwwEgyzzzzTInGnZOTY+69914zdOhQd/v48eONJJOcnFyqMQ0YMMBUrVrV48/OGGOmT59uJJkDBw4YY4x57rnnTLVq1a47PqAi4xQVUIFVr15dDz/8cKH2r776Sj169FB4eLh8fHzk5+enhIQESVJqaup1t/ujH/1IdevWdb8PDAzUD37wA3399dfX7etwOPT44497tDVr1syj7+bNmxUcHKz27dt71HXv3v26279i7ty5at68uQIDA+Xr6ys/Pz+tX7/eY35r1qxRYGCg+9RdUdasWSNJ+vWvf13ifZdE165dC7Xl5eVp0qRJio6Olr+/v3x9feXv76/PP/+80Lh/8IMf6JFHHinVPv/+97/roYceUkREhPLy8tyvDh06SLq87pLUsmVLnT17Vt27d9df//pXnTp16iZmCpRPBBygAqtdu3ahtnPnzik+Pl7bt2/XxIkTtWnTJu3cuVMrVqyQdPmC1+upUaNGobaAgIAS9Q0KClJgYGChvpcuXXK/P336tMLCwgr1LaqtKK+//rp+9atfKTY2VsuXL9e2bdu0c+dOtW/f3mOMJ0+eVEREhPvUXVFOnjwpHx8fhYeHl2jfJVXUn82wYcP0yiuvqEuXLvrb3/6m7du3a+fOnbr//vsLjftG7oz69ttv9be//U1+fn4erx/+8IeS5A4ySUlJeuedd/T111+ra9euqlWrlmJjY5WcnHyDswXKH67BASqwou7K2bBhg44fP65Nmza5j9pIly9aLS9q1KihHTt2FGp3uVwl6r9o0SK1bdtWc+bM8WjPysryeF+zZk3961//UkFBwTVDTs2aNZWfny+Xy1VkKLkiICCg0IXS0uWwVpSi/mwWLVqkZ555RpMmTfJoP3XqlKpVq+YxpmPHjl1zLNcSGhqqZs2a6be//W2Rn0dERLj/vU+fPurTp4/Onz+vjz/+WGPHjlWnTp302WefqV69eqXeN1DecAQHsMyVL9aAgACP9j/84Q9lMZwiJSQkKCsry3166IolS5aUqL/D4Sg0v08++UQpKSkebR06dNClS5eKvSvryumbq8PS1erXr69PPvnEo23Dhg3XvXX/euP+xz/+oW+++abQmD777LMSXRD+fZ06ddJ///tf3XPPPWrRokWh1/cDzhVVqlRRhw4dNGbMGOXk5OjAgQOl2idQXnEEB7BMXFycqlevroEDB2rs2LHy8/PT+++/r3379pX10Nx69eqlN954Qz179tTEiRPVsGFDrVmzRv/85z8lqdhTStLlL/IJEyZo7NixSkhI0KFDhzR+/HhFRUV53A7dvXt3LViwQAMHDtShQ4f00EMPqaCgQNu3b1eTJk301FNPKT4+XklJSZo4caK+/fZbderUSQEBAdqzZ4+CgoL0/PPPS7p8WueVV17Rq6++qoSEBB08eFAzZ86U0+ks8bw7deqkhQsXqnHjxmrWrJl2796t3/3ud4VORw0ZMkRLly5V586dNWrUKLVs2VIXL17U5s2b1alTJz300ENFbn/8+PFKTk5WXFycBg8erEaNGunSpUs6fPiwVq9erblz56pOnTrq37+/KleurAcffFC1a9eWy+XS5MmT5XQ69eMf/7jE8wHKMwIOYJkaNWroH//4h4YPH66ePXuqSpUq6ty5s5YuXarmzZuX9fAkXT5qsGHDBg0ZMkQjRoyQw+FQYmKiZs+erY4dO3qcrinKmDFjdOHCBc2fP1/Tpk1TdHS05s6dq5UrV3o8l8fX11erV6/W5MmTtXjxYs2YMUPBwcG6//77PS5wXrhwoZo3b6758+dr4cKFqly5sqKjoz2eC/P//t//U2ZmphYuXKjp06erZcuWWrZsmTp37lzieb/55pvy8/PT5MmTde7cOTVv3lwrVqzQyy+/7FEXHBysf/3rXxo3bpzefvtt/eY3v1H16tX14x//WM8+++w1t1+7dm3t2rVLEyZM0O9+9zsdO3ZMwcHBioqKUvv27VW9enVJUnx8vBYuXKhly5bpzJkzCg0N1U9+8hO99957qlmzZonnA5RnPMkYQLkxadIkvfzyyzpy5Ai/fgDATeEIDoAyMXPmTElS48aNlZubqw0bNuitt95Sz549CTcAbhoBB0CZCAoK0htvvKHDhw8rOztbdevW1ciRIwudrgGAG8EpKgAAYB1uEwcAANYh4AAAAOsQcAAAgHXuyIuMCwoKdPz4cQUHBxf5OHUAAFD+GGOUlZV13d8xJ92hAef48eOKjIws62EAAIAbcPTo0es+TuKODDjBwcGSLi9QSEhIGY8GAACURGZmpiIjI93f48W5IwPOldNSISEhBBwAACqYklxewkXGAADAOgQcAABgHQIOAACwzh15DQ4A3ImMMcrLy1N+fn5ZDwW4Jj8/P/n4+Nz0dgg4AHAHyMnJUXp6ui5cuFDWQwGK5XA4VKdOHVWtWvWmtkPAAQDLFRQUKC0tTT4+PoqIiJC/vz8POUW5ZIzRyZMndezYMd177703dSSHgAMAlsvJyVFBQYEiIyMVFBRU1sMBilWzZk0dPnxYubm5NxVwuMgYAO4Q13u0PVAe3Kqji/y0AwAA6xBwAACAdQg4AIASyc+XNm2SFi++/M+KeLd527ZtNWTIkBLXHz58WA6HQ3v37vXamOAdXGQMALiuFSukF16Qjh37v7Y6daQ335SefPLW7+9612H06tVLCxcuLPV2V6xYIT8/vxLXR0ZGKj09XaGhoaXeF8oWAQcAUKwVK6Sf/1wyxrP9m28ut3/wwa0POenp6e5/X7p0qV599VUdOnTI3Va5cmWP+tzc3BIFl7vuuqtU4/Dx8VF4eHip+tgiJydH/v7+ZT2MG8YpKgDANeXnXz5yc3W4kf6vbciQW3+6Kjw83P1yOp1yOBzu95cuXVK1atW0bNkytW3bVoGBgVq0aJFOnz6t7t27q06dOgoKCtJ9992nxYsXe2z36lNU9evX16RJk9S3b18FBwerbt26evvtt92fX32KatOmTXI4HFq/fr1atGihoKAgxcXFeYQvSZo4caJq1aql4OBg/fKXv9SoUaP0ox/96Jrzzc/PV79+/RQVFaXKlSurUaNGevPNNwvVvfPOO/rhD3+ogIAA1a5dW88995z7s7Nnz+rZZ59VWFiYAgMD1bRpU/3973+XJI0bN67Q/mfMmKH69eu73/fu3VtdunTR5MmTFRERoR/84AeSpEWLFqlFixYKDg5WeHi4evTooRMnTnhs68CBA3rssccUEhKi4OBgxcfH68svv9THH38sPz8/uVwuj/rhw4erTZs211yPW4GAAwC4pi1bPE9LXc0Y6ejRy3W328iRIzV48GClpqaqXbt2unTpkmJiYvT3v/9d//3vf/Xss88qKSlJ27dvL3Y7r732mlq0aKE9e/Zo0KBB+tWvfqVPP/202D5jxozRa6+9pl27dsnX11d9+/Z1f/b+++/rt7/9raZOnardu3erbt26mjNnTrHbKygoUJ06dbRs2TIdPHhQr776ql566SUtW7bMXTNnzhz9+te/1rPPPqv9+/dr1apVatiwobt/hw4dtHXrVi1atEgHDx7UlClTSv0cmfXr1ys1NVXJycnucJSTk6MJEyZo3759+vDDD5WWlqbevXu7+3zzzTdq06aNAgMDtWHDBu3evVt9+/ZVXl6e2rRpowYNGuhPf/qTuz4vL0+LFi1Snz59SjW2UjN3oIyMDCPJZGRklPVQAMDrLl68aA4ePGguXrxY6r5//rMxl2NM8a8//9kLA/+fBQsWGKfT6X6flpZmJJkZM2Zct2/Hjh3N8OHD3e8TEhLMCy+84H5fr14907NnT/f7goICU6tWLTNnzhyPfe3Zs8cYY8zGjRuNJPPRRx+5+/zjH/8wktzrGxsba3796197jOPBBx80999/f0mnbIwxZtCgQaZr167u9xEREWbMmDFF1v7zn/80lSpVMocOHSry87Fjxxba/xtvvGHq1avnft+rVy8TFhZmsrOzix3Xjh07jCSTlZVljDFm9OjRJioqyuTk5BRZP3XqVNOkSRP3+w8//NBUrVrVnDt3rsj64n5eS/P9zREcAMA11a59a+tupRYtWni8z8/P129/+1s1a9ZMNWrUUNWqVbVu3TodOXKk2O00a9bM/e9XToVdfQqmuD61/zf5K30OHTqkli1betRf/b4oc+fOVYsWLVSzZk1VrVpV8+bNc4/9xIkTOn78uH76058W2Xfv3r2qU6eO+7TSjbrvvvsKXXezZ88ede7cWfXq1VNwcLDatm0rSe6x7d27V/Hx8de8Bqp379764osvtG3bNkmXT7P94he/UJUqVW5qrNdDwAEAXFN8/OW7pa51U5PDIUVGXq673a7+gnzttdf0xhtvaMSIEdqwYYP27t2rdu3aKScnp9jtXP3F7HA4VFBQUOI+V+74+n6fq+8CM0VdxPQ9y5Yt09ChQ9W3b1+tW7dOe/fuVZ8+fdxjv/qi6qtd7/NKlSoVGkNubm6huqvX9Pz580pMTFTVqlW1aNEi7dy5UytXrpSkEo+tVq1aevzxx7VgwQKdOHFCq1ev9jil5y0EHADANfn4XL4VXCoccq68nzHjcl1Z27Jlizp37qyePXvq/vvvV4MGDfT555/f9nE0atRIO3bs8GjbtWtXsX22bNmiuLg4DRo0SA888IAaNmyoL7/80v15cHCw6tevr/Xr1xfZv1mzZjp27Jg+++yzIj+vWbOmXC6XR8gpybN9Pv30U506dUpTpkxRfHy8GjduXOjoVrNmzbRly5YiA9MVv/zlL7VkyRL94Q9/0D333KMHH3zwuvu+WQQcAECxnnzy8q3gd9/t2V6njnduEb9RDRs2VHJysrZu3arU1FQNGDCg0N07t8Pzzz+v+fPn691339Xnn3+uiRMn6pNPPin22T4NGzbUrl279M9//lOfffaZXnnlFe3cudOjZty4cXrttdf01ltv6fPPP9d//vMf/f73v5ckJSQkqE2bNuratauSk5OVlpamNWvWaO3atZIu3z128uRJTZs2TV9++aVmzZqlNWvWXHcudevWlb+/v37/+9/rq6++0qpVqzRhwgSPmueee06ZmZl66qmntGvXLn3++ef605/+5HFnWbt27eR0OjVx4kTvX1z8PwQcAMB1PfmkdPiwtHGj9Oc/X/5nWlr5CTeS9Morr6h58+Zq166d2rZtq/DwcHXp0uW2j+Ppp5/W6NGj9eKLL6p58+buu44CAwOv2WfgwIF68skn1a1bN8XGxur06dMaNGiQR02vXr00Y8YMzZ49Wz/84Q/VqVMnjyNUy5cv149//GN1795d0dHRGjFihPL/d/9+kyZNNHv2bM2aNUv333+/duzYoRdffPG6c6lZs6YWLlyov/zlL4qOjtaUKVM0ffp0j5oaNWpow4YNOnfunBISEhQTE6N58+Z5nMarVKmSevfurfz8fD3zzDMlWseb5TDXOzFooczMTDmdTmVkZCgkJKSshwMAXnXp0iWlpaUpKiqq2C9ZeM+jjz6q8PBwj9ul7zT9+/fXt99+q1WrVhVbV9zPa2m+v3mSMQAAt9CFCxc0d+5ctWvXTj4+Plq8eLE++ugjJScnl/XQykRGRoZ27typ999/X3/9619v234JOAAA3EIOh0OrV6/WxIkTlZ2drUaNGmn58uV65JFHynpoZaJz587asWOHBgwYoEcfffS27ZeAAwDALVS5cmV99NFHZT2McmPTpk1lsl8uMgYAANYh4ADAHeIOvKcEFdCt+jkl4ACA5a7crnvhwoUyHglwfVeekFzaXxR6Na7BAQDL+fj4qFq1au4n0AYFBRX70DmgrBQUFOjkyZMKCgqSr+/NRRQCDgDcAcLDwyXpur9EEihrlSpVUt26dW86hBNwAOAO4HA4VLt2bdWqVavY3xkElDV/f39VqnTzV9AQcADgDuLj43PT1zYAFQEXGQMAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOrcl4MyePVtRUVEKDAxUTEyMtmzZUmz95s2bFRMTo8DAQDVo0EBz5869Zu2SJUvkcDjUpUuXWzxqAABQUXk94CxdulRDhgzRmDFjtGfPHsXHx6tDhw46cuRIkfVpaWnq2LGj4uPjtWfPHr300ksaPHiwli9fXqj266+/1osvvqj4+HhvTwMAAFQgDmOM8eYOYmNj1bx5c82ZM8fd1qRJE3Xp0kWTJ08uVD9y5EitWrVKqamp7raBAwdq3759SklJcbfl5+crISFBffr00ZYtW3T27Fl9+OGHJRpTZmamnE6nMjIyFBIScuOTAwAAt01pvr+9egQnJydHu3fvVmJiokd7YmKitm7dWmSflJSUQvXt2rXTrl27lJub624bP368atasqX79+l13HNnZ2crMzPR4AQAAe3k14Jw6dUr5+fkKCwvzaA8LC5PL5Sqyj8vlKrI+Ly9Pp06dkiT9+9//1vz58zVv3rwSjWPy5MlyOp3uV2Rk5A3MBgAAVBS35SJjh8Ph8d4YU6jtevVX2rOystSzZ0/NmzdPoaGhJdr/6NGjlZGR4X4dPXq0lDMAAAAVia83Nx4aGiofH59CR2tOnDhR6CjNFeHh4UXW+/r6qkaNGjpw4IAOHz6sxx9/3P15QUGBJMnX11eHDh3SPffc49E/ICBAAQEBt2JKAACgAvDqERx/f3/FxMQoOTnZoz05OVlxcXFF9mndunWh+nXr1qlFixby8/NT48aNtX//fu3du9f9euKJJ/TQQw9p7969nH4CAADePYIjScOGDVNSUpJatGih1q1b6+2339aRI0c0cOBASZdPH33zzTd67733JF2+Y2rmzJkaNmyY+vfvr5SUFM2fP1+LFy+WJAUGBqpp06Ye+6hWrZokFWoHAAB3Jq8HnG7duun06dMaP3680tPT1bRpU61evVr16tWTJKWnp3s8EycqKkqrV6/W0KFDNWvWLEVEROitt95S165dvT1UAABgCa8/B6c84jk4AABUPOXmOTgAAABlgYADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALDObQk4s2fPVlRUlAIDAxUTE6MtW7YUW79582bFxMQoMDBQDRo00Ny5cz0+nzdvnuLj41W9enVVr15djzzyiHbs2OHNKQAAgArE6wFn6dKlGjJkiMaMGaM9e/YoPj5eHTp00JEjR4qsT0tLU8eOHRUfH689e/bopZde0uDBg7V8+XJ3zaZNm9S9e3dt3LhRKSkpqlu3rhITE/XNN994ezoAAKACcBhjjDd3EBsbq+bNm2vOnDnutiZNmqhLly6aPHlyofqRI0dq1apVSk1NdbcNHDhQ+/btU0pKSpH7yM/PV/Xq1TVz5kw988wz1x1TZmamnE6nMjIyFBIScgOzAgAAt1tpvr+9egQnJydHu3fvVmJiokd7YmKitm7dWmSflJSUQvXt2rXTrl27lJubW2SfCxcuKDc3V3fddVeRn2dnZyszM9PjBQAA7OXVgHPq1Cnl5+crLCzMoz0sLEwul6vIPi6Xq8j6vLw8nTp1qsg+o0aN0t13361HHnmkyM8nT54sp9PpfkVGRt7AbAAAQEVxWy4ydjgcHu+NMYXarldfVLskTZs2TYsXL9aKFSsUGBhY5PZGjx6tjIwM9+vo0aOlnQIAAKhAfL258dDQUPn4+BQ6WnPixIlCR2muCA8PL7Le19dXNWrU8GifPn26Jk2apI8++kjNmjW75jgCAgIUEBBwg7MAAAAVjVeP4Pj7+ysmJkbJycke7cnJyYqLiyuyT+vWrQvVr1u3Ti1atJCfn5+77Xe/+50mTJigtWvXqkWLFrd+8AAAoMLy+imqYcOG6Y9//KPeeecdpaamaujQoTpy5IgGDhwo6fLpo+/f+TRw4EB9/fXXGjZsmFJTU/XOO+9o/vz5evHFF90106ZN08svv6x33nlH9evXl8vlksvl0rlz57w9HQAAUAF49RSVJHXr1k2nT5/W+PHjlZ6erqZNm2r16tWqV6+eJCk9Pd3jmThRUVFavXq1hg4dqlmzZikiIkJvvfWWunbt6q6ZPXu2cnJy9POf/9xjX2PHjtW4ceO8PSUAAFDOef05OOURz8EBAKDiKTfPwQEAACgLBBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDq3JeDMnj1bUVFRCgwMVExMjLZs2VJs/ebNmxUTE6PAwEA1aNBAc+fOLVSzfPlyRUdHKyAgQNHR0Vq5cqW3hg8AACoYrwecpUuXasiQIRozZoz27Nmj+Ph4dejQQUeOHCmyPi0tTR07dlR8fLz27Nmjl156SYMHD9by5cvdNSkpKerWrZuSkpK0b98+JSUl6Re/+IW2b9/u7ekAAIAKwGGMMd7cQWxsrJo3b645c+a425o0aaIuXbpo8uTJhepHjhypVatWKTU11d02cOBA7du3TykpKZKkbt26KTMzU2vWrHHXtG/fXtWrV9fixYsLbTM7O1vZ2dnu95mZmYqMjFRGRoZCQkJuyTwBAIB3ZWZmyul0luj726tHcHJycrR7924lJiZ6tCcmJmrr1q1F9klJSSlU365dO+3atUu5ubnF1lxrm5MnT5bT6XS/IiMjb3RKAACgAvBqwDl16pTy8/MVFhbm0R4WFiaXy1VkH5fLVWR9Xl6eTp06VWzNtbY5evRoZWRkuF9Hjx690SkBAIAKwPd27MThcHi8N8YUarte/dXtpdlmQECAAgICSjVmAABQcXn1CE5oaKh8fHwKHVk5ceJEoSMwV4SHhxdZ7+vrqxo1ahRbc61tAgCAO4tXA46/v79iYmKUnJzs0Z6cnKy4uLgi+7Ru3bpQ/bp169SiRQv5+fkVW3OtbQIAgDuL109RDRs2TElJSWrRooVat26tt99+W0eOHNHAgQMlXb4+5ptvvtF7770n6fIdUzNnztSwYcPUv39/paSkaP78+R53R73wwgtq06aNpk6dqs6dO+uvf/2rPvroI/3rX//y9nQAAEAF4PWA061bN50+fVrjx49Xenq6mjZtqtWrV6tevXqSpPT0dI9n4kRFRWn16tUaOnSoZs2apYiICL311lvq2rWruyYuLk5LlizRyy+/rFdeeUX33HOPli5dqtjYWG9PBwAAVABefw5OeVSa++gBAED5UG6egwMAAFAWCDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOt4NeCcOXNGSUlJcjqdcjqdSkpK0tmzZ4vtY4zRuHHjFBERocqVK6tt27Y6cOCA+/PvvvtOzz//vBo1aqSgoCDVrVtXgwcPVkZGhjenAgAAKhCvBpwePXpo7969Wrt2rdauXau9e/cqKSmp2D7Tpk3T66+/rpkzZ2rnzp0KDw/Xo48+qqysLEnS8ePHdfz4cU2fPl379+/XwoULtXbtWvXr18+bUwEAABWIwxhjvLHh1NRURUdHa9u2bYqNjZUkbdu2Ta1bt9ann36qRo0aFepjjFFERISGDBmikSNHSpKys7MVFhamqVOnasCAAUXu6y9/+Yt69uyp8+fPy9fX97pjy8zMlNPpVEZGhkJCQm5ilgAA4HYpzfe3147gpKSkyOl0usONJLVq1UpOp1Nbt24tsk9aWppcLpcSExPdbQEBAUpISLhmH0nuiV4r3GRnZyszM9PjBQAA7OW1gONyuVSrVq1C7bVq1ZLL5bpmH0kKCwvzaA8LC7tmn9OnT2vChAnXPLojSZMnT3ZfB+R0OhUZGVnSaQAAgAqo1AFn3Lhxcjgcxb527dolSXI4HIX6G2OKbP++qz+/Vp/MzEw99thjio6O1tixY6+5vdGjRysjI8P9Onr0aEmmCgAAKqjrX7Byleeee05PPfVUsTX169fXJ598om+//bbQZydPnix0hOaK8PBwSZeP5NSuXdvdfuLEiUJ9srKy1L59e1WtWlUrV66Un5/fNccTEBCggICAYscMAADsUeqAExoaqtDQ0OvWtW7dWhkZGdqxY4datmwpSdq+fbsyMjIUFxdXZJ+oqCiFh4crOTlZDzzwgCQpJydHmzdv1tSpU911mZmZateunQICArRq1SoFBgaWdhoAAMBiXrsGp0mTJmrfvr369++vbdu2adu2berfv786derkcQdV48aNtXLlSkmXT00NGTJEkyZN0sqVK/Xf//5XvXv3VlBQkHr06CHp8pGbxMREnT9/XvPnz1dmZqZcLpdcLpfy8/O9NR0AAFCBlPoITmm8//77Gjx4sPuuqCeeeEIzZ870qDl06JDHQ/pGjBihixcvatCgQTpz5oxiY2O1bt06BQcHS5J2796t7du3S5IaNmzosa20tDTVr1/fizMCAAAVgdeeg1Oe8RwcAAAqnnLxHBwAAICyQsABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFjHqwHnzJkzSkpKktPplNPpVFJSks6ePVtsH2OMxo0bp4iICFWuXFlt27bVgQMHrlnboUMHORwOffjhh7d+AgAAoELyasDp0aOH9u7dq7Vr12rt2rXau3evkpKSiu0zbdo0vf7665o5c6Z27typ8PBwPfroo8rKyipUO2PGDDkcDm8NHwAAVFC+3tpwamqq1q5dq23btik2NlaSNG/ePLVu3VqHDh1So0aNCvUxxmjGjBkaM2aMnnzySUnSu+++q7CwMP35z3/WgAED3LX79u3T66+/rp07d6p27dremgYAAKiAvHYEJyUlRU6n0x1uJKlVq1ZyOp3aunVrkX3S0tLkcrmUmJjobgsICFBCQoJHnwsXLqh79+6aOXOmwsPDrzuW7OxsZWZmerwAAIC9vBZwXC6XatWqVai9Vq1acrlc1+wjSWFhYR7tYWFhHn2GDh2quLg4de7cuURjmTx5svs6IKfTqcjIyJJOAwAAVEClDjjjxo2Tw+Eo9rVr1y5JKvL6GGPMda+bufrz7/dZtWqVNmzYoBkzZpR4zKNHj1ZGRob7dfTo0RL3BQAAFU+pr8F57rnn9NRTTxVbU79+fX3yySf69ttvC3128uTJQkdorrhyusnlcnlcV3PixAl3nw0bNujLL79UtWrVPPp27dpV8fHx2rRpU6HtBgQEKCAgoNgxAwAAe5Q64ISGhio0NPS6da1bt1ZGRoZ27Nihli1bSpK2b9+ujIwMxcXFFdknKipK4eHhSk5O1gMPPCBJysnJ0ebNmzV16lRJ0qhRo/TLX/7So999992nN954Q48//nhppwMAACzktbuomjRpovbt26t///76wx/+IEl69tln1alTJ487qBo3bqzJkyfrZz/7mRwOh4YMGaJJkybp3nvv1b333qtJkyYpKChIPXr0kHT5KE9RFxbXrVtXUVFR3poOAACoQLwWcCTp/fff1+DBg913RT3xxBOaOXOmR82hQ4eUkZHhfj9ixAhdvHhRgwYN0pkzZxQbG6t169YpODjYm0MFAAAWcRhjTFkP4nbLzMyU0+lURkaGQkJCyno4AACgBErz/c3vogIAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADr+Jb1AMqCMUaSlJmZWcYjAQAAJXXle/vK93hx7siAk5WVJUmKjIws45EAAIDSysrKktPpLLbGYUoSgyxTUFCg48ePKzg4WA6Ho6yHU+YyMzMVGRmpo0ePKiQkpKyHYy3W+fZgnW8f1vr2YJ3/jzFGWVlZioiIUKVKxV9lc0cewalUqZLq1KlT1sMod0JCQu74/3huB9b59mCdbx/W+vZgnS+73pGbK7jIGAAAWIeAAwAArEPAgQICAjR27FgFBASU9VCsxjrfHqzz7cNa3x6s8425Iy8yBgAAduMIDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBw7gBnzpxRUlKSnE6nnE6nkpKSdPbs2WL7GGM0btw4RUREqHLlymrbtq0OHDhwzdoOHTrI4XDoww8/vPUTqCC8sc7fffednn/+eTVq1EhBQUGqW7euBg8erIyMDC/PpnyZPXu2oqKiFBgYqJiYGG3ZsqXY+s2bNysmJkaBgYFq0KCB5s6dW6hm+fLlio6OVkBAgKKjo7Vy5UpvDb/CuNXrPG/ePMXHx6t69eqqXr26HnnkEe3YscObU6gQvPHzfMWSJUvkcDjUpUuXWzzqCsjAeu3btzdNmzY1W7duNVu3bjVNmzY1nTp1KrbPlClTTHBwsFm+fLnZv3+/6datm6ldu7bJzMwsVPv666+bDh06GElm5cqVXppF+eeNdd6/f7958sknzapVq8wXX3xh1q9fb+69917TtWvX2zGlcmHJkiXGz8/PzJs3zxw8eNC88MILpkqVKubrr78usv6rr74yQUFB5oUXXjAHDx408+bNM35+fuaDDz5w12zdutX4+PiYSZMmmdTUVDNp0iTj6+trtm3bdrumVe54Y5179OhhZs2aZfbs2WNSU1NNnz59jNPpNMeOHbtd0yp3vLHOVxw+fNjcfffdJj4+3nTu3NnLMyn/CDiWO3jwoJHk8T/ulJQUI8l8+umnRfYpKCgw4eHhZsqUKe62S5cuGafTaebOnetRu3fvXlOnTh2Tnp5+Rwccb6/z9y1btsz4+/ub3NzcWzeBcqxly5Zm4MCBHm2NGzc2o0aNKrJ+xIgRpnHjxh5tAwYMMK1atXK//8UvfmHat2/vUdOuXTvz1FNP3aJRVzzeWOer5eXlmeDgYPPuu+/e/IArKG+tc15ennnwwQfNH//4R9OrVy8CjjGGU1SWS0lJkdPpVGxsrLutVatWcjqd2rp1a5F90tLS5HK5lJiY6G4LCAhQQkKCR58LFy6oe/fumjlzpsLDw703iQrAm+t8tYyMDIWEhMjX1/7flZuTk6Pdu3d7rJEkJSYmXnONUlJSCtW3a9dOu3btUm5ubrE1xa27zby1zle7cOGCcnNzddddd92agVcw3lzn8ePHq2bNmurXr9+tH3gFRcCxnMvlUq1atQq116pVSy6X65p9JCksLMyjPSwszKPP0KFDFRcXp86dO9/CEVdM3lzn7zt9+rQmTJigAQMG3OSIK4ZTp04pPz+/VGvkcrmKrM/Ly9OpU6eKrbnWNm3nrXW+2qhRo3T33XfrkUceuTUDr2C8tc7//ve/NX/+fM2bN887A6+gCDgV1Lhx4+RwOIp97dq1S5LkcDgK9TfGFNn+fVd//v0+q1at0oYNGzRjxoxbM6FyqqzX+fsyMzP12GOPKTo6WmPHjr2JWVU8JV2j4uqvbi/tNu8E3ljnK6ZNm6bFixdrxYoVCgwMvAWjrbhu5TpnZWWpZ8+emjdvnkJDQ2/9YCsw+49xW+q5557TU089VWxN/fr19cknn+jbb78t9NnJkycL/a3giiunm1wul2rXru1uP3HihLvPhg0b9OWXX6patWoefbt27ar4+Hht2rSpFLMpv8p6na/IyspS+/btVbVqVa1cuVJ+fn6lnUqFFBoaKh8fn0J/uy1qja4IDw8vst7X11c1atQotuZa27Sdt9b5iunTp2vSpEn66KOP1KxZs1s7+ArEG+t84MABHT58WI8//rj784KCAkmSr6+vDh06pHvuuecWz6SCKKNrf3CbXLn4dfv27e62bdu2leji16lTp7rbsrOzPS5+TU9PN/v37/d4STJvvvmm+eqrr7w7qXLIW+tsjDEZGRmmVatWJiEhwZw/f957kyinWrZsaX71q195tDVp0qTYizKbNGni0TZw4MBCFxl36NDBo6Z9+/Z3/EXGt3qdjTFm2rRpJiQkxKSkpNzaAVdQt3qdL168WOj/xZ07dzYPP/yw2b9/v8nOzvbORCoAAs4doH379qZZs2YmJSXFpKSkmPvuu6/Q7cuNGjUyK1ascL+fMmWKcTqdZsWKFWb//v2me/fu17xN/ArdwXdRGeOddc7MzDSxsbHmvvvuM1988YVJT093v/Ly8m7r/MrKldtq58+fbw4ePGiGDBliqlSpYg4fPmyMMWbUqFEmKSnJXX/lttqhQ4eagwcPmvnz5xe6rfbf//638fHxMVOmTDGpqalmypQp3CbuhXWeOnWq8ff3Nx988IHHz25WVtZtn1954Y11vhp3UV1GwLkDnD592jz99NMmODjYBAcHm6efftqcOXPGo0aSWbBggft9QUGBGTt2rAkPDzcBAQGmTZs2Zv/+/cXu504PON5Y540bNxpJRb7S0tJuz8TKgVmzZpl69eoZf39/07x5c7N582b3Z7169TIJCQke9Zs2bTIPPPCA8ff3N/Xr1zdz5swptM2//OUvplGjRsbPz880btzYLF++3NvTKPdu9TrXq1evyJ/dsWPH3obZlF/e+Hn+PgLOZQ5j/ne1EgAAgCW4iwoAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1vn/s4/UJu4oFy0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGxCAYAAABvIsx7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvv0lEQVR4nO3deXRUVYLH8V+RhAqBpIjEJEQCBMEAsigwBMJEsAfCIgoDjsiSxg1BR5HFAVxaGFC2FqUdNkWU7h4FbAMOx8YMIIsoYQmytUSONkFQUrIIlTRLCMmdP5iUFgkhQSohl+/nnHfk3br3vnvvidSPt8VhjDECAACwSLXKHgAAAMC1RsABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEs53A4yrRt2LDhVx1n0qRJcjgcV9V2w4YN12QMVe3YAPwnsLIHAMC/0tPTffanTJmi9evXa926dT7lzZs3/1XHeeyxx9SjR4+ratumTRulp6f/6jEAQBECDmC5Dh06+OzffPPNqlatWrHyS505c0YhISFlPk69evVUr169qxpjWFjYFccDAOXBJSoA6tKli1q0aKHPPvtMiYmJCgkJ0SOPPCJJWrZsmZKTk1W3bl3VqFFDzZo104QJE3T69GmfPkq6RNWwYUP17t1baWlpatOmjWrUqKGmTZvqnXfe8alX0mWihx56SLVq1dK3336rXr16qVatWoqNjdXYsWOVl5fn0/7777/X/fffr9DQUNWuXVuDBw/W9u3b5XA4tHjx4qtak5UrV6pjx44KCQlRaGiounXrVuxs2LFjx/T4448rNjZWTqdTN998szp16qS1a9d66+zcuVO9e/dWZGSknE6nYmJidM899+j777/31jHGaN68ebrjjjtUo0YNhYeH6/7779eBAwd8jleWvgBcxBkcAJKk7OxsDRkyROPGjdPUqVNVrdrFf/9888036tWrl0aNGqWaNWvq66+/1owZM7Rt27Zil7lKsnv3bo0dO1YTJkxQVFSU3n77bT366KNq3Lix7rrrrlLb5ufn67777tOjjz6qsWPH6rPPPtOUKVPkcrn00ksvSZJOnz6tu+++Wz/99JNmzJihxo0bKy0tTQMGDLjqtXj//fc1ePBgJScna8mSJcrLy9PMmTPVpUsXffrpp/rnf/5nSVJKSoq+/PJLvfLKK7rtttt06tQpffnllzpx4oR3bN26dVNcXJzmzp2rqKgoud1urV+/Xrm5ud7jDR8+XIsXL9bIkSM1Y8YM/fTTT5o8ebISExO1e/duRUVFlbkvAP/PALihDB061NSsWdOnrHPnzkaS+fTTT0ttW1hYaPLz883GjRuNJLN7927vZxMnTjSX/pXSoEEDExwcbL777jtv2dmzZ81NN91khg8f7i1bv369kWTWr1/vM05J5oMPPvDps1evXiY+Pt67P3fuXCPJfPLJJz71hg8fbiSZd999t9Q5XXrsgoICExMTY1q2bGkKCgq89XJzc01kZKRJTEz0ltWqVcuMGjXqsn1nZGQYSeajjz66bJ309HQjycyaNcun/PDhw6ZGjRpm3LhxZe4LwM+4RAVAkhQeHq7f/OY3xcoPHDigQYMGKTo6WgEBAQoKClLnzp0lSZmZmVfs94477lD9+vW9+8HBwbrtttv03XffXbGtw+HQvffe61PWqlUrn7YbN25UaGhosRucBw4ceMX+S7J//34dOXJEKSkp3rNYklSrVi31799fW7Zs0ZkzZyRJ7du31+LFi/Xyyy9ry5Ytys/P9+mrcePGCg8P1/jx47VgwQLt27ev2PE+/vhjORwODRkyRBcuXPBu0dHRat26tfeyXVn6AvAzAg4ASVLdunWLlf3jH/9QUlKStm7dqpdfflkbNmzQ9u3btXz5cknS2bNnr9hvnTp1ipU5nc4ytQ0JCVFwcHCxtufOnfPunzhxQlFRUcXallRWFkWXl0paj5iYGBUWFurkyZOSLt6fNHToUL399tvq2LGjbrrpJv32t7+V2+2WJLlcLm3cuFF33HGHnn/+ed1+++2KiYnRxIkTvWHoxx9/lDFGUVFRCgoK8tm2bNmi48ePl7kvAD/jHhwAklTiO2zWrVunI0eOaMOGDd6zNpJ06tSpChxZ6erUqaNt27YVKy8KGVfTn3TxnqRLHTlyRNWqVVN4eLgkKSIiQrNnz9bs2bN16NAhrVy5UhMmTNDRo0eVlpYmSWrZsqWWLl0qY4z27NmjxYsXa/LkyapRo4YmTJigiIgIORwObdq0SU6ns9gxf1l2pb4A/IwzOAAuqyj0XPrF++abb1bGcErUuXNn5ebm6pNPPvEpX7p06VX1Fx8fr1tuuUXvv/++jDHe8tOnTys1NdX7ZNWl6tevr6eeekrdunXTl19+Wexzh8Oh1q1b6/XXX1ft2rW9dXr37i1jjH744Qe1a9eu2NayZcsy9wXgZ5zBAXBZiYmJCg8P14gRIzRx4kQFBQXpvffe0+7duyt7aF5Dhw7V66+/riFDhujll19W48aN9cknn+h///d/JcnnPpqyqFatmmbOnKnBgwerd+/eGj58uPLy8vT73/9ep06d0vTp0yVJHo9Hd999twYNGqSmTZsqNDRU27dvV1pamvr16yfp4v018+bNU9++fdWoUSMZY7R8+XKdOnVK3bp1kyR16tRJjz/+uB5++GFlZGTorrvuUs2aNZWdna3PP/9cLVu21BNPPFGmvgD8jIAD4LLq1Kmjv/71rxo7dqyGDBmimjVrqk+fPlq2bJnatGlT2cOTJNWsWVPr1q3TqFGjNG7cODkcDiUnJ2vevHnq1auXateuXe4+Bw0apJo1a2ratGkaMGCAAgIC1KFDB61fv16JiYmSLt4snZCQoD//+c86ePCg8vPzVb9+fY0fP17jxo2TJDVp0kS1a9fWzJkzdeTIEVWvXl3x8fFavHixhg4d6j3em2++qQ4dOujNN9/UvHnzVFhYqJiYGHXq1Ent27cvV18ALnKYX56DBQBLTJ06VS+++KIOHTp01W9YBlB1cQYHQJU3Z84cSVLTpk2Vn5+vdevW6Y033tCQIUMIN8ANioADoMoLCQnR66+/roMHDyovL897qejFF1+s7KEBqCRcogIAANbhMXEAAGAdAg4AALAOAQcAAFjnhrzJuLCwUEeOHFFoaGiJr6cHAADXH2OMcnNzFRMTc8WXeN6QAefIkSOKjY2t7GEAAICrcPjw4Su+AuKGDDihoaGSLi5QWFhYJY8GAACURU5OjmJjY73f46W5IQNO0WWpsLAwAg4AAFVMWW4v4SZjAABgHQIOAACwDgEHAABY54a8BwcAUPmMMbpw4YIKCgoqeyi4jgQFBSkgIOBX90PAAQBUuPPnzys7O1tnzpyp7KHgOuNwOFSvXj3VqlXrV/VDwAEAVKjCwkJlZWUpICBAMTExql69Oi9dhaSLZ/WOHTum77//Xk2aNPlVZ3IIOACACnX+/HkVFhYqNjZWISEhlT0cXGduvvlmHTx4UPn5+b8q4HCTMQCgUlzpVfu4MV2rs3n8dAEAAOsQcAAAgHUIOACAKqugQNqwQVqy5OJ/q+IT5126dNGoUaPKXP/gwYNyOBzatWuX38YkSRs2bJDD4dCpU6f8ehx/4SZjAECVtHy59Mwz0vff/1xWr570hz9I/fpd++Nd6d6QoUOHavHixeXud/ny5QoKCipz/djYWGVnZysiIqLcx7qREHAAAFXO8uXS/fdLxviW//DDxfIPP7z2ISc7O9v752XLlumll17S/v37vWU1atTwqZ+fn1+m4HLTTTeVaxwBAQGKjo4uV5sbEZeoAABVSkHBxTM3l4Yb6eeyUaOu/eWq6Oho7+ZyueRwOLz7586dU+3atfXBBx+oS5cuCg4O1n//93/rxIkTGjhwoOrVq6eQkBC1bNlSS5Ys8en30ktUDRs21NSpU/XII48oNDRU9evX11tvveX9/NJLVEWXkj799FO1a9dOISEhSkxM9AlfkvTyyy8rMjJSoaGheuyxxzRhwgTdcccd5VqD1NRU3X777XI6nWrYsKFmzZrl8/m8efPUpEkTBQcHKyoqSvfff7/3sw8//FAtW7ZUjRo1VKdOHXXt2lWnT58u1/HLg4ADAKhSNm3yvSx1KWOkw4cv1qto48eP18iRI5WZmanu3bvr3Llzatu2rT7++GP97W9/0+OPP66UlBRt3bq11H5mzZqldu3aaefOnXryySf1xBNP6Ouvvy61zQsvvKBZs2YpIyNDgYGBeuSRR7yfvffee3rllVc0Y8YM7dixQ/Xr19f8+fPLNbcdO3bogQce0IMPPqi9e/dq0qRJ+t3vfue9LJeRkaGRI0dq8uTJ2r9/v9LS0nTXXXdJunj2a+DAgXrkkUeUmZmpDRs2qF+/fjIlpdRrxdyAPB6PkWQ8Hk9lDwUAbjhnz541+/btM2fPnr2q9u+/b8zFGFP69v7713jgv/Duu+8al8vl3c/KyjKSzOzZs6/YtlevXmbs2LHe/c6dO5tnnnnGu9+gQQMzZMgQ735hYaGJjIw08+fP9znWzp07jTHGrF+/3kgya9eu9bb561//aiR51zghIcH8+7//u884OnXqZFq3bn3ZcRb1e/LkSWOMMYMGDTLdunXzqfMf//Efpnnz5sYYY1JTU01YWJjJyckp1teOHTuMJHPw4MHLHq9IaT8f5fn+5gwOAKBKqVv32ta7ltq1a+ezX1BQoFdeeUWtWrVSnTp1VKtWLa1evVqHDh0qtZ9WrVp5/1x0Kezo0aNlblP3/ydf1Gb//v1q3769T/1L968kMzNTnTp18inr1KmTvvnmGxUUFKhbt25q0KCBGjVqpJSUFL333nve3zXWunVr/cu//Itatmypf/u3f9PChQt18uTJch2/vAg4AIAqJSnp4tNSl3uoyeGQYmMv1qtoNWvW9NmfNWuWXn/9dY0bN07r1q3Trl271L17d50/f77Ufi69OdnhcKiwsLDMbYqe+Pplm0ufAjPlvDxkjCm1j9DQUH355ZdasmSJ6tatq5deekmtW7fWqVOnFBAQoDVr1uiTTz5R8+bN9V//9V+Kj49XVlZWucZQHgQcAECVEhBw8VFwqXjIKdqfPftivcq2adMm9enTR0OGDFHr1q3VqFEjffPNNxU+jvj4eG3bts2nLCMjo1x9NG/eXJ9//rlP2ebNm3Xbbbd5f2dUYGCgunbtqpkzZ2rPnj06ePCg1q1bJ+liwOrUqZP+8z//Uzt37lT16tW1YsWKXzGr0vGYOACgyunX7+Kj4CW9B2f2bP+8B+dqNG7cWKmpqdq8ebPCw8P12muvye12q1mzZhU6jqefflrDhg1Tu3btlJiYqGXLlmnPnj1q1KhRmfsYO3as/umf/klTpkzRgAEDlJ6erjlz5mjevHmSpI8//lgHDhzQXXfdpfDwcK1atUqFhYWKj4/X1q1b9emnnyo5OVmRkZHaunWrjh075td1IOAAAKqkfv2kPn0uPi2VnX3xnpukpOvjzE2R3/3ud8rKylL37t0VEhKixx9/XH379pXH46nQcQwePFgHDhzQs88+q3PnzumBBx7QQw89VOysTmnatGmjDz74QC+99JKmTJmiunXravLkyXrooYckSbVr19by5cs1adIknTt3Tk2aNNGSJUt0++23KzMzU5999plmz56tnJwcNWjQQLNmzVLPnj39NGPJYcp7Ec4COTk5crlc8ng8CgsLq+zhAMAN5dy5c8rKylJcXJyCg4Mrezg3rG7duik6Olp//vOfK3soPkr7+SjP9zdncAAAsNyZM2e0YMECde/eXQEBAVqyZInWrl2rNWvWVPbQ/IaAAwCA5RwOh1atWqWXX35ZeXl5io+PV2pqqrp27VrZQ/MbAg4AAJarUaOG1q5dW9nDqFA8Jg4AAKxDwAEAVIob8BkXlMG1+rkg4AAAKlTRG3eLXuMP/FLRW54DfuXz/tyDAwCoUAEBAapdu7b39ySFhIQU+xUAuDEVFhbq2LFjCgkJUWDgr4soBBwAQIWLjo6WpCv+AknceKpVq6b69ev/6tBLwAEAVDiHw6G6desqMjJS+fn5lT0cXEeqV6+uatV+/R00BBwAQKUJCAj41fdaACXhJmMAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWKdCAs68efMUFxen4OBgtW3bVps2bSq1/saNG9W2bVsFBwerUaNGWrBgwWXrLl26VA6HQ3379r3GowYAAFWV3wPOsmXLNGrUKL3wwgvauXOnkpKS1LNnTx06dKjE+llZWerVq5eSkpK0c+dOPf/88xo5cqRSU1OL1f3uu+/07LPPKikpyd/TAAAAVYjDGGP8eYCEhAS1adNG8+fP95Y1a9ZMffv21bRp04rVHz9+vFauXKnMzExv2YgRI7R7926lp6d7ywoKCtS5c2c9/PDD2rRpk06dOqWPPvqoTGPKycmRy+WSx+NRWFjY1U8OAABUmPJ8f/v1DM758+e1Y8cOJScn+5QnJydr8+bNJbZJT08vVr979+7KyMhQfn6+t2zy5Mm6+eab9eijj15xHHl5ecrJyfHZAACAvfwacI4fP66CggJFRUX5lEdFRcntdpfYxu12l1j/woULOn78uCTpiy++0KJFi7Rw4cIyjWPatGlyuVzeLTY29ipmAwAAqooKucnY4XD47BtjipVdqX5ReW5uroYMGaKFCxcqIiKiTMd/7rnn5PF4vNvhw4fLOQMAAFCVBPqz84iICAUEBBQ7W3P06NFiZ2mKREdHl1g/MDBQderU0VdffaWDBw/q3nvv9X5eWFgoSQoMDNT+/ft16623+rR3Op1yOp3XYkoAAKAK8OsZnOrVq6tt27Zas2aNT/maNWuUmJhYYpuOHTsWq7969Wq1a9dOQUFBatq0qfbu3atdu3Z5t/vuu0933323du3axeUnAADg3zM4kjRmzBilpKSoXbt26tixo9566y0dOnRII0aMkHTx8tEPP/ygP/3pT5IuPjE1Z84cjRkzRsOGDVN6eroWLVqkJUuWSJKCg4PVokULn2PUrl1bkoqVAwCAG5PfA86AAQN04sQJTZ48WdnZ2WrRooVWrVqlBg0aSJKys7N93okTFxenVatWafTo0Zo7d65iYmL0xhtvqH///v4eKgAAsITf34NzPeI9OAAAVD3XzXtwAAAAKgMBBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgnQoJOPPmzVNcXJyCg4PVtm1bbdq0qdT6GzduVNu2bRUcHKxGjRppwYIFPp8vXLhQSUlJCg8PV3h4uLp27apt27b5cwoAAKAK8XvAWbZsmUaNGqUXXnhBO3fuVFJSknr27KlDhw6VWD8rK0u9evVSUlKSdu7cqeeff14jR45Uamqqt86GDRs0cOBArV+/Xunp6apfv76Sk5P1ww8/+Hs6AACgCnAYY4w/D5CQkKA2bdpo/vz53rJmzZqpb9++mjZtWrH648eP18qVK5WZmektGzFihHbv3q309PQSj1FQUKDw8HDNmTNHv/3tb684ppycHLlcLnk8HoWFhV3FrAAAQEUrz/e3X8/gnD9/Xjt27FBycrJPeXJysjZv3lxim/T09GL1u3fvroyMDOXn55fY5syZM8rPz9dNN91U4ud5eXnKycnx2QAAgL38GnCOHz+ugoICRUVF+ZRHRUXJ7XaX2MbtdpdY/8KFCzp+/HiJbSZMmKBbbrlFXbt2LfHzadOmyeVyebfY2NirmA0AAKgqKuQmY4fD4bNvjClWdqX6JZVL0syZM7VkyRItX75cwcHBJfb33HPPyePxeLfDhw+XdwoAAKAKCfRn5xEREQoICCh2tubo0aPFztIUiY6OLrF+YGCg6tSp41P+6quvaurUqVq7dq1atWp12XE4nU45nc6rnAUAAKhq/HoGp3r16mrbtq3WrFnjU75mzRolJiaW2KZjx47F6q9evVrt2rVTUFCQt+z3v/+9pkyZorS0NLVr1+7aDx4AAFRZfr9ENWbMGL399tt65513lJmZqdGjR+vQoUMaMWKEpIuXj3755NOIESP03XffacyYMcrMzNQ777yjRYsW6dlnn/XWmTlzpl588UW98847atiwodxut9xut/7xj3/4ezoAAKAK8OslKkkaMGCATpw4ocmTJys7O1stWrTQqlWr1KBBA0lSdna2zztx4uLitGrVKo0ePVpz585VTEyM3njjDfXv399bZ968eTp//rzuv/9+n2NNnDhRkyZN8veUAADAdc7v78G5HvEeHAAAqp7r5j04AAAAlYGAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYp0ICzrx58xQXF6fg4GC1bdtWmzZtKrX+xo0b1bZtWwUHB6tRo0ZasGBBsTqpqalq3ry5nE6nmjdvrhUrVvhr+AAAoIrxe8BZtmyZRo0apRdeeEE7d+5UUlKSevbsqUOHDpVYPysrS7169VJSUpJ27typ559/XiNHjlRqaqq3Tnp6ugYMGKCUlBTt3r1bKSkpeuCBB7R161Z/TwcAAFQBDmOM8ecBEhIS1KZNG82fP99b1qxZM/Xt21fTpk0rVn/8+PFauXKlMjMzvWUjRozQ7t27lZ6eLkkaMGCAcnJy9Mknn3jr9OjRQ+Hh4VqyZEmxPvPy8pSXl+fdz8nJUWxsrDwej8LCwq7JPAEAgH/l5OTI5XKV6fvbr2dwzp8/rx07dig5OdmnPDk5WZs3by6xTXp6erH63bt3V0ZGhvLz80utc7k+p02bJpfL5d1iY2OvdkoAAKAK8GvAOX78uAoKChQVFeVTHhUVJbfbXWIbt9tdYv0LFy7o+PHjpda5XJ/PPfecPB6Pdzt8+PDVTgkAAFQBgRVxEIfD4bNvjClWdqX6l5aXp0+n0ymn01muMQMAgKrLr2dwIiIiFBAQUOzMytGjR4udgSkSHR1dYv3AwEDVqVOn1DqX6xMAANxY/BpwqlevrrZt22rNmjU+5WvWrFFiYmKJbTp27Fis/urVq9WuXTsFBQWVWudyfQIAgBuL3y9RjRkzRikpKWrXrp06duyot956S4cOHdKIESMkXbw/5ocfftCf/vQnSRefmJozZ47GjBmjYcOGKT09XYsWLfJ5OuqZZ57RXXfdpRkzZqhPnz76n//5H61du1aff/65v6cDAACqAL8HnAEDBujEiROaPHmysrOz1aJFC61atUoNGjSQJGVnZ/u8EycuLk6rVq3S6NGjNXfuXMXExOiNN95Q//79vXUSExO1dOlSvfjii/rd736nW2+9VcuWLVNCQoK/pwMAAKoAv78H53pUnufoAQDA9eG6eQ8OAABAZSDgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACs49eAc/LkSaWkpMjlcsnlciklJUWnTp0qtY0xRpMmTVJMTIxq1KihLl266KuvvvJ+/tNPP+npp59WfHy8QkJCVL9+fY0cOVIej8efUwEAAFWIXwPOoEGDtGvXLqWlpSktLU27du1SSkpKqW1mzpyp1157TXPmzNH27dsVHR2tbt26KTc3V5J05MgRHTlyRK+++qr27t2rxYsXKy0tTY8++qg/pwIAAKoQhzHG+KPjzMxMNW/eXFu2bFFCQoIkacuWLerYsaO+/vprxcfHF2tjjFFMTIxGjRql8ePHS5Ly8vIUFRWlGTNmaPjw4SUe6y9/+YuGDBmi06dPKzAw8Ipjy8nJkcvlksfjUVhY2K+YJQAAqCjl+f722xmc9PR0uVwub7iRpA4dOsjlcmnz5s0ltsnKypLb7VZycrK3zOl0qnPnzpdtI8k70cuFm7y8POXk5PhsAADAXn4LOG63W5GRkcXKIyMj5Xa7L9tGkqKionzKo6KiLtvmxIkTmjJlymXP7kjStGnTvPcBuVwuxcbGlnUaAACgCip3wJk0aZIcDkepW0ZGhiTJ4XAUa2+MKbH8ly79/HJtcnJydM8996h58+aaOHHiZft77rnn5PF4vNvhw4fLMlUAAFBFXfmGlUs89dRTevDBB0ut07BhQ+3Zs0c//vhjsc+OHTtW7AxNkejoaEkXz+TUrVvXW3706NFibXJzc9WjRw/VqlVLK1asUFBQ0GXH43Q65XQ6Sx0zAACwR7kDTkREhCIiIq5Yr2PHjvJ4PNq2bZvat28vSdq6das8Ho8SExNLbBMXF6fo6GitWbNGd955pyTp/Pnz2rhxo2bMmOGtl5OTo+7du8vpdGrlypUKDg4u7zQAAIDF/HYPTrNmzdSjRw8NGzZMW7Zs0ZYtWzRs2DD17t3b5wmqpk2basWKFZIuXpoaNWqUpk6dqhUrVuhvf/ubHnroIYWEhGjQoEGSLp65SU5O1unTp7Vo0SLl5OTI7XbL7XaroKDAX9MBAABVSLnP4JTHe++9p5EjR3qfirrvvvs0Z84cnzr79+/3eUnfuHHjdPbsWT355JM6efKkEhIStHr1aoWGhkqSduzYoa1bt0qSGjdu7NNXVlaWGjZs6McZAQCAqsBv78G5nvEeHAAAqp7r4j04AAAAlYWAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwjl8DzsmTJ5WSkiKXyyWXy6WUlBSdOnWq1DbGGE2aNEkxMTGqUaOGunTpoq+++uqydXv27CmHw6GPPvro2k8AAABUSX4NOIMGDdKuXbuUlpamtLQ07dq1SykpKaW2mTlzpl577TXNmTNH27dvV3R0tLp166bc3NxidWfPni2Hw+Gv4QMAgCoq0F8dZ2ZmKi0tTVu2bFFCQoIkaeHCherYsaP279+v+Pj4Ym2MMZo9e7ZeeOEF9evXT5L0xz/+UVFRUXr//fc1fPhwb93du3frtdde0/bt21W3bl1/TQMAAFRBfjuDk56eLpfL5Q03ktShQwe5XC5t3ry5xDZZWVlyu91KTk72ljmdTnXu3NmnzZkzZzRw4EDNmTNH0dHRVxxLXl6ecnJyfDYAAGAvvwUct9utyMjIYuWRkZFyu92XbSNJUVFRPuVRUVE+bUaPHq3ExET16dOnTGOZNm2a9z4gl8ul2NjYsk4DAABUQeUOOJMmTZLD4Sh1y8jIkKQS748xxlzxvplLP/9lm5UrV2rdunWaPXt2mcf83HPPyePxeLfDhw+XuS0AAKh6yn0PzlNPPaUHH3yw1DoNGzbUnj179OOPPxb77NixY8XO0BQputzkdrt97qs5evSot826dev097//XbVr1/Zp279/fyUlJWnDhg3F+nU6nXI6naWOGQAA2KPcASciIkIRERFXrNexY0d5PB5t27ZN7du3lyRt3bpVHo9HiYmJJbaJi4tTdHS01qxZozvvvFOSdP78eW3cuFEzZsyQJE2YMEGPPfaYT7uWLVvq9ddf17333lve6QAAAAv57SmqZs2aqUePHho2bJjefPNNSdLjjz+u3r17+zxB1bRpU02bNk3/+q//KofDoVGjRmnq1Klq0qSJmjRpoqlTpyokJESDBg2SdPEsT0k3FtevX19xcXH+mg4AAKhC/BZwJOm9997TyJEjvU9F3XfffZozZ45Pnf3798vj8Xj3x40bp7Nnz+rJJ5/UyZMnlZCQoNWrVys0NNSfQwUAABZxGGNMZQ+iouXk5Mjlcsnj8SgsLKyyhwMAAMqgPN/f/C4qAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsA4BBwAAWIeAAwAArEPAAQAA1iHgAAAA6xBwAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGAdAg4AALAOAQcAAFiHgAMAAKxDwAEAANYh4AAAAOsQcAAAgHUIOAAAwDoEHAAAYB0CDgAAsE5gZQ+gMhhjJEk5OTmVPBIAAFBWRd/bRd/jpbkhA05ubq4kKTY2tpJHAgAAyis3N1cul6vUOg5TlhhkmcLCQh05ckShoaFyOByVPZxKl5OTo9jYWB0+fFhhYWGVPRxrsc4Vg3WuOKx1xWCdf2aMUW5urmJiYlStWul32dyQZ3CqVaumevXqVfYwrjthYWE3/P88FYF1rhisc8VhrSsG63zRlc7cFOEmYwAAYB0CDgAAsA4BB3I6nZo4caKcTmdlD8VqrHPFYJ0rDmtdMVjnq3ND3mQMAADsxhkcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeDcAE6ePKmUlBS5XC65XC6lpKTo1KlTpbYxxmjSpEmKiYlRjRo11KVLF3311VeXrduzZ085HA599NFH134CVYQ/1vmnn37S008/rfj4eIWEhKh+/foaOXKkPB6Pn2dzfZk3b57i4uIUHBystm3batOmTaXW37hxo9q2bavg4GA1atRICxYsKFYnNTVVzZs3l9PpVPPmzbVixQp/Db/KuNbrvHDhQiUlJSk8PFzh4eHq2rWrtm3b5s8pVAn++HkusnTpUjkcDvXt2/caj7oKMrBejx49TIsWLczmzZvN5s2bTYsWLUzv3r1LbTN9+nQTGhpqUlNTzd69e82AAQNM3bp1TU5OTrG6r732munZs6eRZFasWOGnWVz//LHOe/fuNf369TMrV6403377rfn0009NkyZNTP/+/StiSteFpUuXmqCgILNw4UKzb98+88wzz5iaNWua7777rsT6Bw4cMCEhIeaZZ54x+/btMwsXLjRBQUHmww8/9NbZvHmzCQgIMFOnTjWZmZlm6tSpJjAw0GzZsqWipnXd8cc6Dxo0yMydO9fs3LnTZGZmmocffti4XC7z/fffV9S0rjv+WOciBw8eNLfccotJSkoyffr08fNMrn8EHMvt27fPSPL5izs9Pd1IMl9//XWJbQoLC010dLSZPn26t+zcuXPG5XKZBQsW+NTdtWuXqVevnsnOzr6hA46/1/mXPvjgA1O9enWTn59/7SZwHWvfvr0ZMWKET1nTpk3NhAkTSqw/btw407RpU5+y4cOHmw4dOnj3H3jgAdOjRw+fOt27dzcPPvjgNRp11eOPdb7UhQsXTGhoqPnjH//46wdcRflrnS9cuGA6depk3n77bTN06FACjjGGS1SWS09Pl8vlUkJCgresQ4cOcrlc2rx5c4ltsrKy5Ha7lZyc7C1zOp3q3LmzT5szZ85o4MCBmjNnjqKjo/03iSrAn+t8KY/Ho7CwMAUG2v+7cs+fP68dO3b4rJEkJScnX3aN0tPTi9Xv3r27MjIylJ+fX2qd0tbdZv5a50udOXNG+fn5uummm67NwKsYf67z5MmTdfPNN+vRRx+99gOvogg4lnO73YqMjCxWHhkZKbfbfdk2khQVFeVTHhUV5dNm9OjRSkxMVJ8+fa7hiKsmf67zL504cUJTpkzR8OHDf+WIq4bjx4+roKCgXGvkdrtLrH/hwgUdP3681DqX69N2/lrnS02YMEG33HKLunbtem0GXsX4a52/+OILLVq0SAsXLvTPwKsoAk4VNWnSJDkcjlK3jIwMSZLD4SjW3hhTYvkvXfr5L9usXLlS69at0+zZs6/NhK5Tlb3Ov5STk6N77rlHzZs318SJE3/FrKqesq5RafUvLS9vnzcCf6xzkZkzZ2rJkiVavny5goODr8Foq65ruc65ubkaMmSIFi5cqIiIiGs/2CrM/nPclnrqqaf04IMPllqnYcOG2rNnj3788cdinx07dqzYvwqKFF1ucrvdqlu3rrf86NGj3jbr1q3T3//+d9WuXdunbf/+/ZWUlKQNGzaUYzbXr8pe5yK5ubnq0aOHatWqpRUrVigoKKi8U6mSIiIiFBAQUOxftyWtUZHo6OgS6wcGBqpOnTql1rlcn7bz1zoXefXVVzV16lStXbtWrVq1uraDr0L8sc5fffWVDh48qHvvvdf7eWFhoSQpMDBQ+/fv16233nqNZ1JFVNK9P6ggRTe/bt261Vu2ZcuWMt38OmPGDG9ZXl6ez82v2dnZZu/evT6bJPOHP/zBHDhwwL+Tug75a52NMcbj8ZgOHTqYzp07m9OnT/tvEtep9u3bmyeeeMKnrFmzZqXelNmsWTOfshEjRhS7ybhnz54+dXr06HHD32R8rdfZGGNmzpxpwsLCTHp6+rUdcBV1rdf57Nmzxf4u7tOnj/nNb35j9u7da/Ly8vwzkSqAgHMD6NGjh2nVqpVJT0836enppmXLlsUeX46PjzfLly/37k+fPt24XC6zfPlys3fvXjNw4MDLPiZeRDfwU1TG+Gedc3JyTEJCgmnZsqX59ttvTXZ2tne7cOFChc6vshQ9Vrto0SKzb98+M2rUKFOzZk1z8OBBY4wxEyZMMCkpKd76RY/Vjh492uzbt88sWrSo2GO1X3zxhQkICDDTp083mZmZZvr06Twm7od1njFjhqlevbr58MMPfX52c3NzK3x+1wt/rPOleIrqIgLODeDEiRNm8ODBJjQ01ISGhprBgwebkydP+tSRZN59913vfmFhoZk4caKJjo42TqfT3HXXXWbv3r2lHudGDzj+WOf169cbSSVuWVlZFTOx68DcuXNNgwYNTPXq1U2bNm3Mxo0bvZ8NHTrUdO7c2af+hg0bzJ133mmqV69uGjZsaObPn1+sz7/85S8mPj7eBAUFmaZNm5rU1FR/T+O6d63XuUGDBiX+7E6cOLECZnP98sfP8y8RcC5yGPP/dysBAABYgqeoAACAdQg4AADAOgQcAABgHQIOAACwDgEHAABYh4ADAACsQ8ABAADWIeAAAADrEHAAAIB1CDgAAMA6BBwAAGCd/wOeSoOhiBsUoAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history['accuracy']\n",
    "loss = history['loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.title('Training accuracies')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.title('Training losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try the model on a test sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The United States might collapsez .'.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sentence words to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "# The indexes or the unknown word idx\n",
    "sentence_word_idxs = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indices. Note the 1 at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ['the', 'united', 'states', 'might', 'collapsez', '.']\n",
      "Sentence word indexes tensor([358640, 373606, 343335, 245002,      1,    873])\n"
     ]
    }
   ],
   "source": [
    "print('Sentence', sentence)\n",
    "print('Sentence word indexes', sentence_word_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the chunks. Call the variable `sent_chunk_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "sent_chunk_predictions = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 23])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_chunk_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated probabilities of the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2072e-13, 4.7664e-10, 3.5222e-09, 7.4905e-13, 8.2765e-10, 9.5403e-09,\n",
       "        1.0000e+00, 9.4836e-10, 3.9878e-12, 4.5347e-09, 1.2401e-11, 1.8891e-13,\n",
       "        1.4451e-13, 1.7238e-13, 1.3286e-12, 2.0025e-11, 9.5634e-09, 1.7781e-12,\n",
       "        1.2139e-11, 2.6272e-12, 5.4331e-11, 9.5586e-15, 1.6024e-06],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(sent_chunk_predictions[0], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 16, 16, 11, 21, 22])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(F.softmax(sent_chunk_predictions, dim=-1), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply argmax to select the chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: B-NP\n",
      "united: I-NP\n",
      "states: I-NP\n",
      "might: B-VP\n",
      "collapsez /ukn: I-VP\n",
      ".: O\n"
     ]
    }
   ],
   "source": [
    "for word_nbr, chunk_predictions in enumerate(sent_chunk_predictions):\n",
    "    if int(sentence_word_idxs[word_nbr]) in idx2word:\n",
    "        print(idx2word[int(sentence_word_idxs[word_nbr])], end=': ')\n",
    "    else:\n",
    "        print(sentence[word_nbr], '/ukn', end=': ')\n",
    "    print(idx2chunk.get(int(torch.argmax(F.softmax(chunk_predictions, dim=-1), dim=-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_dict = split_rows(test_sentences, column_names)\n",
    "test_dict[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the ${X}$ and ${Y}$ sequences of symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test: ['rockwell', 'said', 'the', 'agreement', 'calls', 'for', 'it', 'to', 'supply', '200', 'additional', 'so-called', 'shipsets', 'for', 'the', 'planes', '.']\n",
      "Y_test ['B-NP', 'B-VP', 'B-NP', 'I-NP', 'B-VP', 'B-SBAR', 'B-NP', 'B-VP', 'I-VP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'B-PP', 'B-NP', 'I-NP', 'O']\n"
     ]
    }
   ],
   "source": [
    "X_test_symbs, Y_test_symbs = build_sequences(test_dict, key_x='form', key_y='chunk')\n",
    "print('X_test:', X_test_symbs[1])\n",
    "print('Y_test', Y_test_symbs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the ${X}$ symbol sequence into an index sequence and pad it. Call the results `X_test_idx` and `X_test_padded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "X_test_idx = []\n",
    "for x in X_test_symbs:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_idx = map(torch.LongTensor, X_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_padded = pad_sequence(X_test_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_padded: tensor([311438, 316957, 358640,  48789,  90494, 152124, 194623, 362305, 349553,\n",
      "         17495,  46648, 337426,      1, 152124, 358640, 287224,    873,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0])\n"
     ]
    }
   ],
   "source": [
    "print('X_test_padded:', X_test_padded[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2012, 70])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the chunks. Call the result `Y_test_hat_probs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "Y_test_hat_probs = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions tensor([[ -7.0167,  -1.3971,  -2.6404,  ...,  -1.0799,  -1.3150,   4.7549],\n",
      "        [ -6.5066,  -4.5663,   1.0363,  ...,  -2.3344,   6.9481,   6.8664],\n",
      "        [-11.6368,  -2.9430,  -2.0845,  ...,  -4.6665,  -7.5087,   6.0453],\n",
      "        ...,\n",
      "        [ 20.2845,  -4.0088,  -3.6083,  ...,  -1.0157,  -5.1971,   0.3017],\n",
      "        [ 18.3593,  -3.3927,  -3.4281,  ...,  -0.7486,  -4.9716,  -0.0463],\n",
      "        [ 16.9232,  -2.7952,  -3.0611,  ...,  -0.5927,  -4.9505,  -0.2680]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Predictions', Y_test_hat_probs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_hat_probs = F.softmax(Y_test_hat_probs, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now predict the whole test set and we store the results in each dictionary with the key `pchunk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent, y_hat_probs in zip(test_dict, Y_test_hat_probs):\n",
    "    sent_len = len(sent)\n",
    "    y_hat_probs = y_hat_probs[:sent_len]\n",
    "    # y_hat = torch.argmax(y_hat_probs, dim=-1) # This statement sometimes predicts 0 (the padding symbol)\n",
    "    y_hat = torch.argmax(y_hat_probs[:, 1:], dim=-1) + 1 # Never predicts 0\n",
    "    for word, ner_hat in zip(sent, y_hat):\n",
    "        word['pchunk'] = idx2chunk.get(int(ner_hat)) \n",
    "        if word['pchunk'] == None:\n",
    "            print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sentence example: `chunk` is the hand annotation and `pchunk` is the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       " {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       " {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       " {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       " {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       " {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR', 'pchunk': 'B-PP'},\n",
       " {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       " {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       " {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       " {'form': '200', 'pos': 'CD', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       " {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       " {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       " {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       " {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       " {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       " {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       " {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the test set in a file to evaluate the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['form', 'pos', 'chunk', 'pchunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(file, corpus_dict, column_names):\n",
    "    \"\"\"\n",
    "    Saves the corpus in a file\n",
    "    :param file:\n",
    "    :param corpus_dict:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    with open(file, 'w', encoding='utf8') as f_out:\n",
    "        i += 1\n",
    "        for sentence in corpus_dict:\n",
    "            sentence_lst = []\n",
    "            for row in sentence:\n",
    "                items = map(lambda x: row.get(x, '_'), column_names)\n",
    "                sentence_lst += ' '.join(items) + '\\n'\n",
    "            sentence_lst += '\\n'\n",
    "            f_out.write(''.join(sentence_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'test_model1.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(outfile, test_dict, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000020716372148"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open(outfile, encoding='utf8').read().splitlines()\n",
    "res = conlleval.evaluate(lines)\n",
    "chunker_score = res['overall']['chunks']['evals']['f1']\n",
    "chunker_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results may slightly vary depending on the run\n",
    "# 0.8650974227443842 lstm nontrainable 15 epochs\n",
    "# 0.8579701751845953 lstm trainable 15 epochs\n",
    "# 0.9015216169521867 lstm bidi nontrainable 15 epochs\n",
    "# 0.9000310655483068 lstm bidi trainable 15 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will carry out experiments with two different recurrent networks: RNN and LSTM. You will also try at least one set of parameters per network, i.e. two experiments, one with a RNN and one with a LSTM. To run a RNN, just replace the LSTM class with RNN. As baseline, a simple solution you consider a starting point, please report the baseline figures from CoNLL 2000: https://aclanthology.org/W00-0726.pdf. \n",
    "\n",
    "In your report, you will present your results in a table like this one:\n",
    "\n",
    "|Method|Parameters|Score|\n",
    "|------|-----|-----|\n",
    "|Baseline|  xx | xx |\n",
    "|RNN|  xx |xx |\n",
    "|RNN |  xx |xx |\n",
    "|LSTM |  xx |xx |\n",
    "|LSTM |  xx |xx |\n",
    "|  Akbik et al.|  xx|xx |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will:\n",
    "1. Write a short individual report on your program. You will describe the architecture your used the different experiments you carried out and your results.\n",
    "2. Read the article, <a href=\"https://www.aclweb.org/anthology/C18-1139\"><i>Contextual String Embeddings for Sequence Labeling</i></a> by Akbik et al. (2018) and outline the main differences between their system and yours. A LSTM is a type of recurrent neural network, while CRF is a sort of beam search. You will tell the performance they reach on the corpus you used in this laboratory.\n",
    "\n",
    "Submit your report as well as your notebook (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, you can either\n",
    "1. Write directly your text in Canvas, or\n",
    "2. Use Latex and Overleaf (www.overleaf.com). This will probably help you structure your text. You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is October 13, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
