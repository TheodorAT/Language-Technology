{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization using Byte-Pair Encoding and a Unigram Language Model\n",
    "\n",
    "Author: Pierre Nugues with help from Marcus Klang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will create a tokenization program to handle subwords.\n",
    "\n",
    "In many scripts from Asia, like Chinese, Korean, or Japanese scripts, tokenization cannot rely on white spaces. The byte-pair encoding and the unigram language model are techniques that are now common in machine translation to carry out a tokenization at a subword level. Subword level tokenization shows better multilingual capabilities.\n",
    "\n",
    "You will follow two papers: \n",
    "* Subword Regularization: _Improving Neural Network Translation Models with Multiple Subword Candidates_ by Kudo (2018) (https://arxiv.org/pdf/1804.10959.pdf) and \n",
    "* _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ by Bostrom and Durrett (2020) (https://aclanthology.org/2020.findings-emnlp.414.pdf). \n",
    "\n",
    "In addition, you will start from a clear and easy-to-understand description in Google’s Neural Machine Translation System: _Bridging the Gap between Human and Machine Translation_ by Wu et al. (2016). (Do not read them now)\n",
    "https://arxiv.org/abs/1609.08144\n",
    "\n",
    "You will use a small corpus make it easier to test and correct your code. Note also that you will use _characters_ and not _bytes_ in this lab as this is simpler to implement. For a complete program, see the link at the end.\n",
    "\n",
    "**In your report, be sure to answer all the questions. Please reuse the section titles of this notebook so that I can check your answers more easily**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an overall description of the subword tokenizers, read Sections 4 (introduction paragraph) and 4.1. in the paper on translation: _Bridging the Gap between Human and Machine Translation_ by Wu et al. (2016), https://arxiv.org/abs/1609.08144.  \n",
    "\n",
    "In your report, in a few lines (10 to 15 lines or so) you will:\n",
    "\n",
    "1. Outline the difference with tokenization as you saw it during the course;\n",
    "2. Imagine how the tokens will be learned (this will developed in the rest of the lab);\n",
    "3. Summarize what could be the advantages for Asian languages, unknown words, and translation.\n",
    "\n",
    "Commenting Sections 4 and 4.1 in your report is **mandatory**. If you are curious, you can read the complete article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding SentencePiece\n",
    "SentencePiece is the combination of BPE and a language model. To be sure you understand how it works, you will first run this code.\n",
    "These are the first cells from a larger program: https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb\n",
    "from Google.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\theo\\anaconda3\\lib\\site-packages (0.1.99)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece\n",
    "!wget https: // raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model from a corpus. Once trained, read the content of `m.vocab`. Be sure to undestand its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    '--input=botchan.txt --model_prefix=m --vocab_size=2000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[386, 315, 27, 52, 1997, 227, 282, 14, 1058, 237, 22, 717, 1060, 443, 27, 67, 38, 20, 14]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode('Tokenization using Byte-Pair Encoding'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁To'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.id_to_piece(386)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " ',',\n",
       " '.',\n",
       " '▁the',\n",
       " '▁I',\n",
       " 's',\n",
       " '▁to',\n",
       " '▁a',\n",
       " '▁and',\n",
       " '▁of',\n",
       " '▁',\n",
       " 'ed',\n",
       " 'ing',\n",
       " '▁in',\n",
       " '▁was',\n",
       " '▁\"',\n",
       " '▁it',\n",
       " 't']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sp.id_to_piece(i) for i in range(20)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁To', 'ke', 'n', 'i', 'z', 'ation', '▁us', 'ing', '▁By', 'te', '-', 'P', 'air', '▁E', 'n', 'c', 'o', 'd', 'ing']\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode('Tokenization using Byte-Pair Encoding', out_type=str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the BPE Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first algorithm to build the subwords from a corpus is a byte-pair encoding (BPE), due to Gage (1994). In the lab, you will first read two sections of more recent articles as they are easier to understand and specifically targeted to natural language processing.\n",
    "\n",
    "Read these two sections:\n",
    "\n",
    "1. Section 3.1 of _Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates_ (https://arxiv.org/pdf/1804.10959.pdf) by Kudo (2018).\n",
    "2. Section 2, algorithm 1 of _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ (https://aclanthology.org/2020.findings-emnlp.414.pdf) by Bostrom and Durrett (2020).\n",
    "\n",
    "In your report, **summarize** (10 to 15 lines or so) with your own words the byte-pair encoding (BPE) algorithm as described by Kudo (2018) and Bostrom and Durrett (2020) (Only BPE and not the unigram language model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now program a byte-pair encoding program in Python. You will do it step by step. The first part will be to extract the subwords from a corpus. Note that you will use the characters, not the bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import tqdm as tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use a small corpus and then, if you have time, test your program on a larger one. Here we take the smallest novel from Selma Lagerlöf in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 65536 bytes written to Selma.zip\n",
      "Downloading: 131072 bytes written to Selma.zip\n",
      "Downloading: 196608 bytes written to Selma.zip\n",
      "Downloading: 262144 bytes written to Selma.zip\n",
      "Downloading: 327680 bytes written to Selma.zip\n",
      "Downloading: 393216 bytes written to Selma.zip\n",
      "Downloading: 458752 bytes written to Selma.zip\n",
      "Downloading: 524288 bytes written to Selma.zip\n",
      "Downloading: 589824 bytes written to Selma.zip\n",
      "Downloading: 655360 bytes written to Selma.zip\n",
      "Downloading: 720896 bytes written to Selma.zip\n",
      "Downloading: 786432 bytes written to Selma.zip\n",
      "Downloading: 851968 bytes written to Selma.zip\n",
      "Downloading: 917504 bytes written to Selma.zip\n",
      "Downloading: 983040 bytes written to Selma.zip\n",
      "Downloading: 1048576 bytes written to Selma.zip\n",
      "Downloading: 1114112 bytes written to Selma.zip\n",
      "Downloading: 1179648 bytes written to Selma.zip\n",
      "Downloading: 1245184 bytes written to Selma.zip\n",
      "Downloading: 1310720 bytes written to Selma.zip\n",
      "Downloading: 1376256 bytes written to Selma.zip\n",
      "Downloading: 1441792 bytes written to Selma.zip\n",
      "Downloading: 1507328 bytes written to Selma.zip\n",
      "Downloading: 1572864 bytes written to Selma.zip\n",
      "Downloading: 1638400 bytes written to Selma.zip\n",
      "Downloading: 1703936 bytes written to Selma.zip\n",
      "Downloading: 1769472 bytes written to Selma.zip\n",
      "Downloading: 1835008 bytes written to Selma.zip\n",
      "Downloading: 1900544 bytes written to Selma.zip\n",
      "Downloading: 1941636 bytes written to Selma.zip\n",
      "Selma.zip donwnloaded.\n",
      "Extracted: Selma/troll.txt\n",
      "Extracted: Selma/kejsaren.txt\n",
      "Extracted: Selma/marbacka.txt\n",
      "Extracted: Selma/herrgard.txt\n",
      "Extracted: Selma/nils.txt\n",
      "Extracted: Selma/osynliga.txt\n",
      "Extracted: Selma/jerusalem.txt\n",
      "Extracted: Selma/bannlyst.txt\n",
      "Extracted: Selma/gosta.txt\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Selma\\\\bannlyst.txt',\n",
       " 'Selma\\\\gosta.txt',\n",
       " 'Selma\\\\herrgard.txt',\n",
       " 'Selma\\\\jerusalem.txt',\n",
       " 'Selma\\\\kejsaren.txt',\n",
       " 'Selma\\\\marbacka.txt',\n",
       " 'Selma\\\\nils.txt',\n",
       " 'Selma\\\\osynliga.txt',\n",
       " 'Selma\\\\troll.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "# Parameters for Selma dataset\n",
    "SELMA_URL = \"https://github.com/pnugues/ilppp/raw/master/programs/corpus/Selma.zip\"\n",
    "\n",
    "SELMA_FILES = [\n",
    "    os.path.join(\"Selma\", fname)\n",
    "    for fname in\n",
    "    [\n",
    "        \"bannlyst.txt\",\n",
    "        \"gosta.txt\",\n",
    "        \"herrgard.txt\",\n",
    "        \"jerusalem.txt\",\n",
    "        \"kejsaren.txt\",\n",
    "        \"marbacka.txt\",\n",
    "        \"nils.txt\",\n",
    "        \"osynliga.txt\",\n",
    "        \"troll.txt\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "def download_and_extract_selma():\n",
    "    \"\"\"Downloads and unpacks Selma.zip\"\"\"\n",
    "\n",
    "    # Download if not all files exist\n",
    "    req = requests.get(SELMA_URL, stream=True)\n",
    "    if req.status_code != 200:\n",
    "        print(\"Failed to download file, got status: \" + req.status_code)\n",
    "        req.close()\n",
    "    else:\n",
    "        with open(\"Selma.zip\", \"wb\") as fd:\n",
    "            written = 0\n",
    "            for chunk in req.iter_content(chunk_size=65536):\n",
    "                fd.write(chunk)\n",
    "                written += len(chunk)\n",
    "                print(\"Downloading: %d bytes written to Selma.zip\" % written)\n",
    "\n",
    "        print(\"Selma.zip donwnloaded.\")\n",
    "        req.close()\n",
    "\n",
    "        selma_zipfile = ZipFile(\"Selma.zip\")\n",
    "        selma_files_to_extract = [zi for zi in selma_zipfile.filelist if not zi.filename.startswith(\n",
    "            \"__\") and zi.filename.endswith(\".txt\")]\n",
    "        for zi in selma_files_to_extract:\n",
    "            selma_zipfile.extract(zi)\n",
    "            print(\"Extracted: \" + zi.filename)\n",
    "\n",
    "        print(\"Done!\")\n",
    "\n",
    "\n",
    "# If not all path exists (all are true), then download\n",
    "if not all([os.path.exists(fname) for fname in SELMA_FILES]):\n",
    "    download_and_extract_selma()\n",
    "else:\n",
    "    print(\"Selma has been downloaded.\")\n",
    "\n",
    "SELMA_FILES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILE_PATH = '../../corpus/Selma.txt'\n",
    "FILE_PATH = 'Selma/herrgard.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the corpus and store it in the `corpus` string variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH, encoding='utf8') as f:\n",
    "    corpus = f.read().strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all the space sequences in `corpus`, including newlines and tabulations, and normalize them as one space. Use the `\\s` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "corpus = re.sub(r\"\\s+\", \" \", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Selma Lagerlöf En herrgårdssägen Bokutgåva Albert Bonniers förlag, Stockholm 1899. I. Det var en skö'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code (one instruction) to split the corpus in a list of characters and store the results in `corpus_l`. This is just a type conversion. Given the input:\n",
    "<pre><span style=\"font-size: 12pt;\">corpus = 'De senaste fem &aring;ren har cirka 25 000 unga'</span></pre>\n",
    "\n",
    "Return:\n",
    "<pre><span style=\"font-size: 12pt;\">corpus_l = ['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', 'e', ' ', 'f', 'e', 'm', ' ', ...]</span></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "corpus_l = list(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'e', 'l', 'm', 'a', ' ', 'L', 'a', 'g', 'e', 'r', 'l', 'ö', 'f', ' ']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_l[:15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the set of characters that will serve as initial subword tokens:\n",
    "\n",
    "1. Write a statement to extract the set of all the characters from `corpus_l`; \n",
    "2. Exclude the space from this set and call the resulting set: `char_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "char_set = set(corpus_l)\n",
    "char_set.remove(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ',', '-', '.', '1', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'X', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', '»', 'Ä', 'Å', 'Ö', 'ä', 'å', 'é', 'ö', '–', '’']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(char_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code from the previous question, write an `initial_vocabulary()` function taking the the `corpus_l` variable as input and returning the the set of all characters appearing in the corpus (the initial character set), deprived from the white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def initial_vocabulary(corpus_l):\n",
    "    vocabulary = set(corpus_l)\n",
    "    vocabulary.remove(\" \")\n",
    "    return sorted(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '1',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'X',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '»',\n",
       " 'Ä',\n",
       " 'Å',\n",
       " 'Ö',\n",
       " 'ä',\n",
       " 'å',\n",
       " 'é',\n",
       " 'ö',\n",
       " '–',\n",
       " '’']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_vocabulary(corpus_l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `pair_count()` function that takes a list of tokens as input, possibly single characters or subword tokens, and that counts the adjacent pairs (bigrams). You will implement these counts as dictionaries: The key will be a pair (tuple) of adjacent symbols and the value, its frequency. Remember that you cannot cross whitespaces, i.e. a pair cannot include a whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input\n",
    "\n",
    "`['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', ...]`\n",
    "count_pairs should return a dictionary: \n",
    "\n",
    "\n",
    "`{('D', 'e'): 1, ('s', 'e'): 1, ('e', 'n'): 1, ('n', 'a'): 1, ...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def pair_count(corpus_l):\n",
    "    vocabulary = initial_vocabulary(corpus_l)\n",
    "    pairs = {}\n",
    "    for i in range(len(corpus_l) - 1):\n",
    "        token1 = corpus_l[i]\n",
    "        token2 = corpus_l[i+1]\n",
    "        if token1 in vocabulary and token2 in vocabulary:\n",
    "            pair = (token1, token2)\n",
    "            if pair not in pairs:\n",
    "                pairs[pair] = 1\n",
    "            else: \n",
    "                pairs[pair] += 1\n",
    "\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pair_count(corpus_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('S', 'e'): 7,\n",
       " ('e', 'l'): 511,\n",
       " ('l', 'm'): 22,\n",
       " ('m', 'a'): 424,\n",
       " ('L', 'a'): 8,\n",
       " ('a', 'g'): 477,\n",
       " ('g', 'e'): 646,\n",
       " ('e', 'r'): 1281,\n",
       " ('r', 'l'): 213,\n",
       " ('l', 'ö'): 70,\n",
       " ('ö', 'f'): 217,\n",
       " ('E', 'n'): 21,\n",
       " ('h', 'e'): 773,\n",
       " ('r', 'r'): 177,\n",
       " ('r', 'g'): 140,\n",
       " ('g', 'å'): 327,\n",
       " ('å', 'r'): 295,\n",
       " ('r', 'd'): 413,\n",
       " ('d', 's'): 83,\n",
       " ('s', 's'): 218,\n",
       " ('s', 'ä'): 205,\n",
       " ('ä', 'g'): 143,\n",
       " ('e', 'n'): 3439,\n",
       " ('B', 'o'): 2,\n",
       " ('o', 'k'): 54,\n",
       " ('k', 'u'): 562,\n",
       " ('u', 't'): 295,\n",
       " ('t', 'g'): 43,\n",
       " ('å', 'v'): 2,\n",
       " ('v', 'a'): 1426,\n",
       " ('A', 'l'): 24,\n",
       " ('l', 'b'): 44,\n",
       " ('b', 'e'): 314,\n",
       " ('r', 't'): 381,\n",
       " ('o', 'n'): 1600,\n",
       " ('n', 'n'): 1070,\n",
       " ('n', 'i'): 231,\n",
       " ('i', 'e'): 22,\n",
       " ('r', 's'): 258,\n",
       " ('f', 'ö'): 980,\n",
       " ('ö', 'r'): 1317,\n",
       " ('l', 'a'): 873,\n",
       " ('g', ','): 192,\n",
       " ('S', 't'): 88,\n",
       " ('t', 'o'): 351,\n",
       " ('o', 'c'): 1223,\n",
       " ('c', 'k'): 794,\n",
       " ('k', 'h'): 23,\n",
       " ('h', 'o'): 1098,\n",
       " ('o', 'l'): 211,\n",
       " ('1', '8'): 1,\n",
       " ('8', '9'): 1,\n",
       " ('9', '9'): 1,\n",
       " ('9', '.'): 1,\n",
       " ('I', '.'): 6,\n",
       " ('D', 'e'): 429,\n",
       " ('e', 't'): 1954,\n",
       " ('a', 'r'): 1970,\n",
       " ('s', 'k'): 835,\n",
       " ('k', 'ö'): 57,\n",
       " ('ö', 'n'): 101,\n",
       " ('h', 'ö'): 223,\n",
       " ('ö', 's'): 77,\n",
       " ('s', 't'): 1632,\n",
       " ('t', 'd'): 3,\n",
       " ('d', 'a'): 626,\n",
       " ('h', 'ä'): 194,\n",
       " ('ä', 'n'): 741,\n",
       " ('n', 'e'): 813,\n",
       " ('e', 'm'): 213,\n",
       " ('m', 'o'): 208,\n",
       " ('o', 't'): 255,\n",
       " ('s', 'l'): 162,\n",
       " ('l', 'u'): 75,\n",
       " ('t', 'e'): 1102,\n",
       " ('a', 'f'): 419,\n",
       " ('t', 'r'): 379,\n",
       " ('r', 'e'): 526,\n",
       " ('t', 't'): 2178,\n",
       " ('t', 'i'): 676,\n",
       " ('i', 'o'): 95,\n",
       " ('t', 'a'): 1180,\n",
       " ('a', 'l'): 722,\n",
       " ('l', 'e'): 887,\n",
       " ('t', '.'): 304,\n",
       " ('P', 'å'): 14,\n",
       " ('d', 'e'): 4016,\n",
       " ('i', 'd'): 484,\n",
       " ('f', 'a'): 194,\n",
       " ('a', 'n'): 2441,\n",
       " ('n', 's'): 453,\n",
       " ('U', 'p'): 9,\n",
       " ('p', 's'): 29,\n",
       " ('s', 'a'): 644,\n",
       " ('ö', 'g'): 125,\n",
       " ('g', 't'): 287,\n",
       " ('t', ','): 409,\n",
       " ('g', 'u'): 32,\n",
       " ('u', 'l'): 353,\n",
       " ('l', 't'): 214,\n",
       " ('t', 'v'): 67,\n",
       " ('v', 'å'): 94,\n",
       " ('å', 'n'): 325,\n",
       " ('i', 'n'): 1047,\n",
       " ('n', 'g'): 1119,\n",
       " ('g', 's'): 109,\n",
       " ('s', 'h'): 10,\n",
       " ('h', 'u'): 160,\n",
       " ('u', 's'): 158,\n",
       " ('s', ','): 45,\n",
       " ('s', 'o'): 632,\n",
       " ('o', 'm'): 1610,\n",
       " ('o', 'd'): 220,\n",
       " ('u', 'n'): 671,\n",
       " ('n', 'd'): 1063,\n",
       " ('l', 'i'): 908,\n",
       " ('i', 'g'): 1139,\n",
       " ('a', 'm'): 353,\n",
       " ('m', 't'): 72,\n",
       " ('p', 'å'): 522,\n",
       " ('i', 't'): 506,\n",
       " ('l', 'å'): 190,\n",
       " ('b', 'o'): 161,\n",
       " ('o', 'r'): 739,\n",
       " ('t', 'k'): 8,\n",
       " ('k', 'a'): 831,\n",
       " ('n', 't'): 331,\n",
       " ('a', 'd'): 1441,\n",
       " ('n', '.'): 485,\n",
       " ('r', 'ä'): 368,\n",
       " ('ä', 't'): 180,\n",
       " ('r', 'u'): 333,\n",
       " ('k', 'i'): 109,\n",
       " ('c', 'h'): 1234,\n",
       " ('e', 'f'): 232,\n",
       " ('f', 'l'): 132,\n",
       " ('m', 'e'): 739,\n",
       " ('n', 'a'): 1027,\n",
       " ('e', 's'): 406,\n",
       " ('a', 's'): 292,\n",
       " ('v', 'i'): 562,\n",
       " ('i', 'l'): 642,\n",
       " ('l', 'd'): 218,\n",
       " ('d', 'v'): 39,\n",
       " ('n', ','): 512,\n",
       " ('v', 'ä'): 357,\n",
       " ('ä', 'x'): 17,\n",
       " ('x', 't'): 9,\n",
       " ('d', 'ä'): 206,\n",
       " ('ä', 'r'): 819,\n",
       " ('r', ','): 244,\n",
       " ('l', 's'): 182,\n",
       " ('s', 'i'): 625,\n",
       " ('k', 'r'): 190,\n",
       " ('ä', 'l'): 348,\n",
       " ('s', 'å'): 595,\n",
       " ('u', 'p'): 197,\n",
       " ('p', 'p'): 322,\n",
       " ('p', 'f'): 13,\n",
       " ('g', 'g'): 74,\n",
       " ('a', 't'): 1639,\n",
       " ('l', 'l'): 1551,\n",
       " ('m', 'r'): 16,\n",
       " ('r', 'a'): 1079,\n",
       " ('f', 'v'): 564,\n",
       " ('v', 'e'): 454,\n",
       " ('r', 'v'): 36,\n",
       " ('u', 'm'): 88,\n",
       " ('n', 'f'): 24,\n",
       " ('n', 'r'): 9,\n",
       " ('t', 'u'): 123,\n",
       " ('u', 'd'): 84,\n",
       " ('d', 'r'): 263,\n",
       " ('a', 'c'): 77,\n",
       " ('g', 'o'): 359,\n",
       " ('n', 'k'): 188,\n",
       " ('f', 'f'): 39,\n",
       " ('f', 'e'): 17,\n",
       " ('e', '.'): 276,\n",
       " ('H', 'a'): 246,\n",
       " ('k', 'e'): 460,\n",
       " ('e', 'd'): 680,\n",
       " ('f', 'i'): 176,\n",
       " ('t', 's'): 127,\n",
       " ('s', 'e'): 298,\n",
       " ('e', 'e'): 21,\n",
       " ('H', 'å'): 1,\n",
       " ('b', 'a'): 231,\n",
       " ('h', 'a'): 1366,\n",
       " ('u', 'k'): 89,\n",
       " ('u', 'r'): 207,\n",
       " ('p', 'a'): 215,\n",
       " ('u', 'g'): 62,\n",
       " ('t', 'ä'): 197,\n",
       " ('d', 'i'): 216,\n",
       " ('k', 'l'): 133,\n",
       " ('l', 'ä'): 238,\n",
       " ('ä', 'd'): 173,\n",
       " ('d', 'd'): 155,\n",
       " ('e', 'k'): 74,\n",
       " ('k', 'v'): 63,\n",
       " ('ä', 'm'): 113,\n",
       " ('ä', 'k'): 37,\n",
       " ('k', 't'): 379,\n",
       " ('e', 'g'): 107,\n",
       " ('g', 'a'): 611,\n",
       " ('m', ','): 133,\n",
       " ('o', 'f'): 75,\n",
       " ('a', ','): 363,\n",
       " ('o', 'p'): 66,\n",
       " ('r', 'i'): 613,\n",
       " ('i', 'f'): 205,\n",
       " ('f', 'b'): 6,\n",
       " ('p', 'r'): 96,\n",
       " ('h', 'y'): 29,\n",
       " ('y', 'l'): 15,\n",
       " ('l', 'o'): 233,\n",
       " ('n', 'ä'): 124,\n",
       " ('ä', 's'): 198,\n",
       " ('b', 'ö'): 96,\n",
       " ('ö', 'c'): 2,\n",
       " ('r', '.'): 179,\n",
       " ('I', 'n'): 272,\n",
       " ('i', 'c'): 254,\n",
       " ('e', ','): 315,\n",
       " ('k', 'o'): 491,\n",
       " ('n', 'o'): 403,\n",
       " ('m', '.'): 156,\n",
       " ('r', 'b'): 50,\n",
       " ('b', 'r'): 167,\n",
       " ('l', ','): 62,\n",
       " ('r', 'k'): 225,\n",
       " ('k', ','): 55,\n",
       " ('f', 'u'): 45,\n",
       " ('i', 'k'): 182,\n",
       " ('h', 'å'): 89,\n",
       " ('g', 'r'): 496,\n",
       " ('r', 'o'): 290,\n",
       " ('y', '.'): 1,\n",
       " ('D', 'u'): 29,\n",
       " ('u', ','): 39,\n",
       " ('H', 'e'): 142,\n",
       " ('j', 'a'): 286,\n",
       " ('m', 'm'): 419,\n",
       " ('l', 'v'): 10,\n",
       " ('g', '.'): 168,\n",
       " ('d', 'u'): 163,\n",
       " ('r', 'å'): 253,\n",
       " ('å', 'k'): 42,\n",
       " ('n', 'å'): 234,\n",
       " ('å', 'g'): 453,\n",
       " ('o', 'b'): 14,\n",
       " ('e', 'h'): 56,\n",
       " ('g', 'l'): 104,\n",
       " ('t', '?'): 23,\n",
       " ('e', 'j'): 591,\n",
       " ('j', ','): 39,\n",
       " ('s', 'n'): 95,\n",
       " ('g', 'ä'): 43,\n",
       " ('t', 'y'): 171,\n",
       " ('y', 's'): 68,\n",
       " ('d', '.'): 118,\n",
       " ('L', 'å'): 7,\n",
       " ('å', 't'): 296,\n",
       " ('d', 'å'): 228,\n",
       " ('o', 'g'): 200,\n",
       " ('b', 'l'): 365,\n",
       " ('t', 'l'): 50,\n",
       " ('l', 'y'): 94,\n",
       " ('d', 'l'): 48,\n",
       " ('n', 'u'): 148,\n",
       " ('J', 'a'): 96,\n",
       " ('m', 'i'): 220,\n",
       " ('t', 'å'): 107,\n",
       " ('u', '.'): 15,\n",
       " ('s', 'j'): 115,\n",
       " ('j', 'ä'): 177,\n",
       " ('l', 'f'): 80,\n",
       " ('f', ':'): 1,\n",
       " ('G', 'u'): 41,\n",
       " ('Å', 'l'): 25,\n",
       " ('r', 'p'): 17,\n",
       " ('y', 'c'): 215,\n",
       " ('å', 'd'): 189,\n",
       " ('m', 'ä'): 107,\n",
       " ('F', 'ö'): 17,\n",
       " ('M', 'i'): 8,\n",
       " ('r', 'f'): 136,\n",
       " ('j', 'u'): 270,\n",
       " ('i', 'h'): 30,\n",
       " ('r', 'ö'): 189,\n",
       " ('f', 'r'): 419,\n",
       " ('m', 'f'): 27,\n",
       " ('h', 'v'): 202,\n",
       " ('r', 'j'): 107,\n",
       " ('j', 'e'): 54,\n",
       " ('n', 'b'): 44,\n",
       " ('t', 'm'): 25,\n",
       " ('h', 'j'): 53,\n",
       " ('l', 'p'): 37,\n",
       " ('n', 'h'): 10,\n",
       " ('l', 'k'): 95,\n",
       " ('l', 'n'): 50,\n",
       " ('p', 't'): 49,\n",
       " ('f', 'å'): 130,\n",
       " ('a', '.'): 265,\n",
       " ('r', 'y'): 81,\n",
       " ('T', 'a'): 8,\n",
       " ('k', 'y'): 103,\n",
       " ('y', 'm'): 27,\n",
       " ('g', 'ö'): 67,\n",
       " ('k', 'n'): 75,\n",
       " ('a', 'p'): 73,\n",
       " ('ö', 'p'): 35,\n",
       " ('p', 'n'): 29,\n",
       " ('f', 'y'): 21,\n",
       " ('y', 'r'): 93,\n",
       " ('r', 'm'): 96,\n",
       " ('r', 'n'): 341,\n",
       " ('a', 'k'): 151,\n",
       " ('s', 'p'): 152,\n",
       " ('p', 'e'): 169,\n",
       " ('å', 'l'): 104,\n",
       " ('g', 'i'): 147,\n",
       " ('F', 'a'): 5,\n",
       " ('v', 'u'): 43,\n",
       " ('l', 'r'): 22,\n",
       " ('f', 'o'): 116,\n",
       " ('t', 'f'): 41,\n",
       " ('m', 'h'): 15,\n",
       " ('M', 'u'): 20,\n",
       " ('y', 't'): 71,\n",
       " ('l', '.'): 56,\n",
       " ('e', 'x'): 6,\n",
       " ('x', 'a'): 11,\n",
       " ('k', 's'): 68,\n",
       " ('f', '.'): 32,\n",
       " ('m', 'g'): 54,\n",
       " ('m', 'u'): 47,\n",
       " ('s', 'r'): 40,\n",
       " ('M', 'e'): 181,\n",
       " ('å', ','): 85,\n",
       " ('u', 'f'): 53,\n",
       " ('i', 's'): 303,\n",
       " ('m', 'd'): 35,\n",
       " ('d', 'ö'): 80,\n",
       " ('ö', 'd'): 95,\n",
       " ('d', ','): 168,\n",
       " ('B', 'e'): 15,\n",
       " ('e', 'a'): 6,\n",
       " ('s', '?'): 4,\n",
       " ('T', 'ä'): 3,\n",
       " ('f', 't'): 166,\n",
       " ('f', ','): 20,\n",
       " ('j', 'o'): 81,\n",
       " ('d', 'b'): 11,\n",
       " ('V', 'ä'): 3,\n",
       " ('m', 'l'): 67,\n",
       " ('u', 'b'): 22,\n",
       " ('b', 'b'): 15,\n",
       " ('d', 't'): 146,\n",
       " ('o', ','): 23,\n",
       " ('H', 'u'): 19,\n",
       " ('r', 'h'): 26,\n",
       " ('s', 'm'): 74,\n",
       " ('m', 'å'): 165,\n",
       " ('H', 'o'): 279,\n",
       " ('m', 'k'): 27,\n",
       " ('t', 'ö'): 29,\n",
       " ('å', 'e'): 15,\n",
       " ('l', 'j'): 90,\n",
       " ('d', 'o'): 83,\n",
       " ('f', 'ä'): 72,\n",
       " ('n', 'y'): 60,\n",
       " ('m', '!'): 8,\n",
       " ('S', 'å'): 41,\n",
       " ('b', 'i'): 39,\n",
       " ('V', 'i'): 27,\n",
       " ('i', 'p'): 22,\n",
       " ('ä', 'c'): 87,\n",
       " ('n', '!'): 13,\n",
       " ('h', 'i'): 32,\n",
       " ('n', 'v'): 12,\n",
       " ('O', 'c'): 138,\n",
       " ('k', '.'): 51,\n",
       " ('D', 'ä'): 24,\n",
       " ('k', '!'): 1,\n",
       " ('n', 'ö'): 51,\n",
       " ('ö', 'j'): 66,\n",
       " ('V', 'a'): 8,\n",
       " ('g', 'n'): 53,\n",
       " ('o', 'ä'): 10,\n",
       " ('n', '?'): 21,\n",
       " ('s', 'v'): 119,\n",
       " ('e', 'p'): 22,\n",
       " ('b', 'ä'): 68,\n",
       " ('i', 'r'): 25,\n",
       " ('p', '.'): 10,\n",
       " ('»', 'Å'): 1,\n",
       " ('g', 'j'): 50,\n",
       " ('H', 'v'): 38,\n",
       " ('t', 'j'): 49,\n",
       " ('l', '?'): 2,\n",
       " ('s', 'u'): 23,\n",
       " ('B', 'a'): 9,\n",
       " ('j', 'l'): 26,\n",
       " ('o', 's'): 115,\n",
       " ('s', 'b'): 20,\n",
       " ('N', 'å'): 7,\n",
       " ('d', 'j'): 65,\n",
       " ('m', 's'): 42,\n",
       " ('G', 'j'): 1,\n",
       " ('Å', 'j'): 2,\n",
       " ('v', 'o'): 90,\n",
       " ('m', 'ö'): 74,\n",
       " ('ö', 't'): 56,\n",
       " ('y', 'n'): 60,\n",
       " ('.', '.'): 12,\n",
       " ('s', 'y'): 59,\n",
       " ('ö', '.'): 6,\n",
       " ('k', 'ä'): 151,\n",
       " ('m', 'n'): 53,\n",
       " ('D', 'å'): 60,\n",
       " ('i', 'm'): 33,\n",
       " ('ö', 'l'): 71,\n",
       " ('b', 'u'): 30,\n",
       " ('e', 'b'): 12,\n",
       " ('A', 't'): 1,\n",
       " ('å', 's'): 89,\n",
       " ('g', 'd'): 59,\n",
       " ('f', 'd'): 45,\n",
       " ('v', 'r'): 12,\n",
       " ('p', ','): 18,\n",
       " ('j', 'ö'): 31,\n",
       " ('J', 'u'): 19,\n",
       " ('g', 'v'): 2,\n",
       " ('p', 'l'): 58,\n",
       " ('ä', 'p'): 33,\n",
       " ('m', 'v'): 16,\n",
       " ('d', 'm'): 7,\n",
       " ('m', 'j'): 8,\n",
       " ('å', '.'): 27,\n",
       " ('B', 'l'): 52,\n",
       " ('ä', 'f'): 52,\n",
       " ('y', 'd'): 48,\n",
       " ('F', 'l'): 10,\n",
       " ('N', 'u'): 23,\n",
       " ('F', 'r'): 17,\n",
       " ('d', 'g'): 28,\n",
       " ('F', 'i'): 5,\n",
       " ('g', 'm'): 3,\n",
       " ('s', 'f'): 22,\n",
       " ('l', '–'): 1,\n",
       " ('d', 'h'): 5,\n",
       " ('m', 'p'): 18,\n",
       " ('d', 'f'): 14,\n",
       " ('d', '?'): 4,\n",
       " ('å', 'f'): 5,\n",
       " ('s', 'c'): 11,\n",
       " ('k', 'j'): 7,\n",
       " ('U', 'n'): 12,\n",
       " ('i', 'b'): 4,\n",
       " ('g', 'k'): 7,\n",
       " ('p', 'u'): 7,\n",
       " ('m', 'y'): 74,\n",
       " ('y', 'g'): 48,\n",
       " ('p', 'h'): 4,\n",
       " ('A', 'k'): 1,\n",
       " ('ö', 'm'): 79,\n",
       " ('t', 'n'): 44,\n",
       " ('e', '!'): 6,\n",
       " ('d', 'k'): 12,\n",
       " ('y', 'k'): 3,\n",
       " ('t', 'b'): 13,\n",
       " ('K', 'o'): 5,\n",
       " ('K', 'a'): 18,\n",
       " ('c', 'i'): 6,\n",
       " ('j', 'd'): 37,\n",
       " ('S', 'j'): 1,\n",
       " ('s', 'g'): 4,\n",
       " ('l', 'h'): 18,\n",
       " ('s', '.'): 37,\n",
       " ('f', 's'): 19,\n",
       " ('e', 'v'): 11,\n",
       " ('d', 'n'): 22,\n",
       " ('O', 'm'): 26,\n",
       " ('o', 'a'): 3,\n",
       " ('M', 'a'): 19,\n",
       " ('C', 'i'): 1,\n",
       " ('r', '!'): 6,\n",
       " ('I', 'b'): 2,\n",
       " ('m', 'b'): 12,\n",
       " ('e', 'c'): 11,\n",
       " ('K', 'u'): 4,\n",
       " ('p', 'm'): 6,\n",
       " ('T', 'r'): 4,\n",
       " ('J', 'o'): 3,\n",
       " ('V', 'å'): 4,\n",
       " ('n', 'l'): 30,\n",
       " ('L', 'ä'): 4,\n",
       " ('y', ','): 1,\n",
       " ('p', 'i'): 30,\n",
       " ('t', '!'): 10,\n",
       " ('e', 'z'): 1,\n",
       " ('z', '?'): 1,\n",
       " (',', 'v'): 1,\n",
       " ('r', '?'): 11,\n",
       " ('Ä', 'r'): 7,\n",
       " ('i', ','): 5,\n",
       " ('f', 'k'): 8,\n",
       " ('e', '?'): 11,\n",
       " ('n', 'p'): 2,\n",
       " ('N', 'e'): 22,\n",
       " ('T', 'å'): 2,\n",
       " ('s', 'ö'): 61,\n",
       " ('ö', 'k'): 65,\n",
       " ('k', ';'): 1,\n",
       " ('o', 'h'): 6,\n",
       " ('N', 'ä'): 5,\n",
       " ('g', 'h'): 24,\n",
       " ('_', '_'): 119,\n",
       " ('I', 'I'): 6,\n",
       " ('ö', 'a'): 3,\n",
       " ('M', 'ä'): 3,\n",
       " ('f', 'n'): 16,\n",
       " ('y', 'p'): 3,\n",
       " ('u', 'x'): 9,\n",
       " ('x', 'i'): 3,\n",
       " ('b', 'y'): 10,\n",
       " ('Ö', 'f'): 1,\n",
       " ('p', 'g'): 2,\n",
       " ('n', 'm'): 5,\n",
       " ('a', 'v'): 7,\n",
       " ('ö', 'b'): 3,\n",
       " ('G', 'a'): 1,\n",
       " ('Ä', 'f'): 1,\n",
       " ('g', 'f'): 59,\n",
       " ('x', 'n'): 2,\n",
       " ('V', 'e'): 5,\n",
       " ('o', 'v'): 6,\n",
       " ('G', 'e'): 15,\n",
       " ('b', 'å'): 19,\n",
       " ('ö', ','): 7,\n",
       " ('y', 'f'): 29,\n",
       " ('a', 'x'): 8,\n",
       " ('x', 'l'): 3,\n",
       " ('N', 'i'): 7,\n",
       " ('o', 'j'): 3,\n",
       " ('H', 'ö'): 10,\n",
       " ('x', 'e'): 9,\n",
       " ('å', 'h'): 1,\n",
       " ('S', 'n'): 2,\n",
       " ('j', '.'): 10,\n",
       " ('R', 'å'): 10,\n",
       " ('D', 'a'): 6,\n",
       " ('O', 'v'): 1,\n",
       " ('b', 'j'): 11,\n",
       " ('S', 'y'): 2,\n",
       " ('k', 'd'): 5,\n",
       " ('L', 'i'): 16,\n",
       " ('G', 'r'): 5,\n",
       " ('k', 'f'): 5,\n",
       " ('G', 'o'): 2,\n",
       " ('l', '!'): 2,\n",
       " ('g', '?'): 8,\n",
       " ('a', '?'): 12,\n",
       " ('m', '?'): 7,\n",
       " ('f', 'h'): 3,\n",
       " ('p', 'o'): 8,\n",
       " ('o', 'u'): 4,\n",
       " ('I', 'V'): 1,\n",
       " ('V', '.'): 2,\n",
       " ('v', 'ö'): 1,\n",
       " ('S', 'o'): 22,\n",
       " ('T', 'ö'): 3,\n",
       " ('T', 'i'): 2,\n",
       " ('p', 'ä'): 13,\n",
       " ('k', 'p'): 7,\n",
       " ('K', 'r'): 2,\n",
       " ('R', 'i'): 1,\n",
       " ('A', 'f'): 2,\n",
       " ('i', 'a'): 8,\n",
       " ('l', 'g'): 7,\n",
       " ('n', 'j'): 3,\n",
       " ('Ö', 'g'): 6,\n",
       " ('Ä', 'n'): 4,\n",
       " ('T', 'v'): 2,\n",
       " ('u', 'c'): 7,\n",
       " ('E', 'g'): 1,\n",
       " ('h', ','): 2,\n",
       " ('U', 's'): 1,\n",
       " ('a', 'b'): 17,\n",
       " ('a', '!'): 2,\n",
       " ('H', 'ä'): 15,\n",
       " ('s', 'd'): 9,\n",
       " ('y', 'ö'): 1,\n",
       " ('ä', 'b'): 2,\n",
       " ('E', 'j'): 1,\n",
       " ('E', 'f'): 2,\n",
       " ('T', 'o'): 2,\n",
       " ('N', 'y'): 1,\n",
       " ('P', 'r'): 5,\n",
       " ('L', 'j'): 3,\n",
       " ('S', 'l'): 2,\n",
       " ('y', 'x'): 1,\n",
       " ('x', 'h'): 1,\n",
       " ('f', 'm'): 2,\n",
       " ('j', 's'): 2,\n",
       " ('R', 'u'): 2,\n",
       " ('ä', 'h'): 3,\n",
       " ('C', 'a'): 2,\n",
       " ('S', 'a'): 2,\n",
       " ('o', '.'): 8,\n",
       " ('A', 'c'): 3,\n",
       " ('d', '!'): 2,\n",
       " ('a', ';'): 1,\n",
       " ('k', 'b'): 4,\n",
       " ('g', '!'): 4,\n",
       " ('R', 'ö'): 1,\n",
       " ('B', 'r'): 1,\n",
       " ('y', 'a'): 3,\n",
       " ('å', 'b'): 2,\n",
       " ('ä', ','): 1,\n",
       " ('f', 'j'): 3,\n",
       " ('d', 'y'): 6,\n",
       " ('M', 'o'): 11,\n",
       " ('p', 'y'): 1,\n",
       " ('k', 'å'): 5,\n",
       " ('t', 'h'): 3,\n",
       " ('B', 'å'): 1,\n",
       " ('F', 'o'): 2,\n",
       " ('u', '!'): 2,\n",
       " ('B', 'ä'): 2,\n",
       " ('l', 'é'): 7,\n",
       " ('é', 'n'): 6,\n",
       " ('K', 'l'): 2,\n",
       " ('A', 'n'): 30,\n",
       " ('G', 'å'): 3,\n",
       " ('u', '?'): 7,\n",
       " ('e', ';'): 1,\n",
       " ('F', 'e'): 1,\n",
       " ('p', 'k'): 3,\n",
       " ('E', 't'): 2,\n",
       " ('V', 'I'): 3,\n",
       " ('D', 'ö'): 5,\n",
       " ('p', 'ö'): 2,\n",
       " ('y', 'i'): 1,\n",
       " ('e', '’'): 1,\n",
       " ('’', 'n'): 1,\n",
       " ('o', 'o'): 1,\n",
       " ('j', 't'): 1,\n",
       " ('y', 'u'): 1,\n",
       " ('s', '-'): 1,\n",
       " ('-', 'k'): 1,\n",
       " ('D', 'i'): 2,\n",
       " ('o', 'e'): 1,\n",
       " ('n', ':'): 4,\n",
       " ('Å', 'n'): 4,\n",
       " ('S', 'ä'): 3,\n",
       " ('å', 'p'): 1,\n",
       " ('k', 'k'): 1,\n",
       " ('a', ':'): 2,\n",
       " ('é', 'e'): 1,\n",
       " ('Å', 't'): 2,\n",
       " ('S', 'k'): 4,\n",
       " ('V', 'o'): 1,\n",
       " ('Å', ','): 1,\n",
       " ('R', 'ä'): 1,\n",
       " ('U', 'r'): 1,\n",
       " ('i', '.'): 5,\n",
       " ('H', 'y'): 2,\n",
       " ('i', '!'): 1,\n",
       " ('K', 'n'): 1,\n",
       " ('P', 'i'): 3,\n",
       " ('å', '?'): 2,\n",
       " ('å', '!'): 1,\n",
       " ('s', '!'): 2,\n",
       " ('ö', 'h'): 1,\n",
       " ('d', 'p'): 1,\n",
       " ('y', 'o'): 1,\n",
       " ('I', 'c'): 2,\n",
       " ('T', 'y'): 4,\n",
       " ('I', 's'): 2,\n",
       " ('p', 'v'): 1,\n",
       " ('K', 'ö'): 1,\n",
       " ('p', '?'): 2,\n",
       " ('U', 't'): 3,\n",
       " ('I', 'X'): 1,\n",
       " ('X', '.'): 2,\n",
       " ('e', ':'): 2,\n",
       " ('K', 'i'): 1,\n",
       " ('P', 'a'): 2,\n",
       " ('j', '?'): 2,\n",
       " ('N', 'o'): 1,\n",
       " ('p', '!'): 1,\n",
       " ('K', 'ä'): 1,\n",
       " ('g', 'b'): 4,\n",
       " ('A', 's'): 2,\n",
       " ('O', 'l'): 8,\n",
       " ('»', 'L'): 2,\n",
       " ('n', '»'): 1,\n",
       " ('»', ','): 1,\n",
       " ('J', 'e'): 1,\n",
       " ('t', ':'): 1,\n",
       " ('p', 'j'): 2,\n",
       " ('v', 'é'): 2,\n",
       " ('é', 'r'): 3,\n",
       " ('å', 'm'): 3,\n",
       " ('x', ','): 1,\n",
       " ('o', 'å'): 1,\n",
       " ('i', 'u'): 1,\n",
       " ('c', 'e'): 2,\n",
       " ('u', 'e'): 2,\n",
       " ('t', '»'): 1,\n",
       " ('»', '.'): 1,\n",
       " ('j', 'k'): 1,\n",
       " ('o', 'z'): 1,\n",
       " ('z', 'a'): 1,\n",
       " ('g', 'y'): 1,\n",
       " ('M', 'ö'): 5,\n",
       " ('n', 'é'): 1,\n",
       " ('h', '!'): 1,\n",
       " ('h', '?'): 1,\n",
       " ('m', ':'): 1,\n",
       " ('t', 'p'): 1,\n",
       " ('g', ':'): 1,\n",
       " ('M', 'å'): 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the most frequent pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "most_freq_pair = max(pairs, key=lambda pair: pairs.get(pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('d', 'e')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(most_freq_pair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The First Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the initial symbols in a `vocabulary` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = initial_vocabulary(corpus_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your most frequent pair to the vocabulary after one iteration as well as the merge operations: `merge_ops`. `merge_ops` will contain the merge operations in the order you created them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_ops = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "most_freq_pair = max(pairs, key=lambda pair: pairs.get(pair))\n",
    "merge_ops.append(most_freq_pair)\n",
    "vocabulary.append(''.join(most_freq_pair))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 'e')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental Construction\n",
    "We will now incrementally build the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `merge_bigrams()` function that takes a list of tokens, `corpus_l`, and a pair of subword tokens `(token_r, token_l)` as input and merges adjacent sequences token_r, token_l into a new token, `token_new`, replacing the sequence `token_r, token_l` in `corpus_l`. Your function will return a new list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input \n",
    "\n",
    "`corpus_l = ['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', ...]`\n",
    "\n",
    "`merge_bigrams(corpus_l, ('e', 'n'))` should return where all the seuquences of 'e' and 'n' have been merged:\n",
    "\n",
    "`['D', 'e', ' ', 's', 'en', 'a', 's', 't', ...]`\n",
    "\n",
    "And reapplying `merge_bigrams(corpus_l, ('s', 'en'))` to this corpus should return\n",
    "\n",
    "`['D', 'e', ' ', 'sen', 'a', 's', 't', ...]`\n",
    "\n",
    "You will apply a greedy algorithm. Given the pair ('a', 'a') and the list ['a', 'a', 'a'], the result will be: ['aa', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def merge_bigrams(corpus_l, pair):\n",
    "    separator = \"<SEPARATOR>\"\n",
    "    new_corpus_l = re.sub(pair[0] + separator + pair[1], ''.join(pair), separator.join(corpus_l))\n",
    "    return new_corpus_l.split(separator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'e', ' ', 's', 'en', 'a', 's', 't', 'e']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test = ['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', 'e']\n",
    "merge_bigrams(corpus_test, ('e', 'n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'e', ' ', 'sen', 'a', 's', 't', 'e']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_bigrams(merge_bigrams(corpus_test, ('e', 'n')), ('s', 'en'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'a']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test = ['a', 'a', 'a']\n",
    "merge_bigrams(corpus_test, ('a', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte Pair Encoding (BPE): Building the Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write now a `BPE()` function following Algorithm 1 in _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ by Bostrom and Durrett (2020). \n",
    "\n",
    "Your function will take `corpus_l` and the vocabulary size `k` as input. This size `k` will correspond to the count of new subwords added to the initial list of symbols. With your initial corpus, you should have 67 found symbols. With `k = 10`, you will add 10 subwords to this initial list. Note that Bostrom and Durrett (2020) define their $k_\\text{Bostrom and Durrett}$ as `k + initial vocabulary`. \n",
    "\n",
    "Return the vocabulary of subword tokens in the form of a list, the initial vocabulary and the subwords you will create, as well as the merge operations in the order you created them\n",
    "\n",
    "You will start from the initial vocabulary and `k` will be the number of symbols you add to this vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def BPE(corpus_l, k):\n",
    "    vocabulary = initial_vocabulary(corpus_l)\n",
    "    merge_ops = []\n",
    "    ...\n",
    "    return vocabulary, merge_ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a vocabulary of 50 subwords in addition to our initial set of symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de en an tt ar st om on ll ör att ch ade ig er ng och var hon et för sk är ck han or na det ne så än in ej un ill den som fv på ed ag li enne henne id ra hade all ing ta "
     ]
    }
   ],
   "source": [
    "vocabulary, merge_ops = BPE(corpus_l, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 50)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary), len(merge_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T', 'ar', 'D', 'll', 'han', 'I', 'li', 'e', '9', 'n', 'ra', 'id', 'ng', 'in', 'så', 'all', 'V', 't', 'ta', 'fv', 'F', 'som', '–', 'A', 'ö', 'u', 'för', 'ör', 'C', 'un', 'Ä', 'ck', 'et', 'R', 'j', '’', 'ch', 'h', 'tt', 'U', 'om', 'y', 'on', ';', 'Ö', 'or', 'ill', 'på', 'Å', 'r', 'K', 'z', 'P', 'an', 'var', 'o', 'k', ',', 'f', 'ade', 'g', 'l', 'och', 'G', '8', 'J', 'L', 'än', 'henne', 'E', '_', 'enne', 'O', 'i', 'B', 'det', 'ag', 's', '-', 'är', 'ä', 'v', 'N', '!', 'p', '?', 'ne', 'er', 'a', 'é', 'att', 'c', 'sk', 'X', 'de', 'm', 'M', 'na', 'H', 'hon', 'en', 'd', 'b', 'å', 'S', '.', 'ig', 'ej', '»', 'ing', '1', 'st', 'x', 'hade', ':', 'ed', 'den'}\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d', 'e'), ('e', 'n'), ('a', 'n'), ('t', 't'), ('a', 'r'), ('s', 't'), ('o', 'm'), ('o', 'n'), ('l', 'l'), ('ö', 'r'), ('a', 'tt'), ('c', 'h'), ('a', 'de'), ('i', 'g'), ('e', 'r'), ('n', 'g'), ('o', 'ch'), ('v', 'ar'), ('h', 'on'), ('e', 't'), ('f', 'ör'), ('s', 'k'), ('ä', 'r'), ('c', 'k'), ('h', 'an'), ('o', 'r'), ('n', 'a'), ('de', 't'), ('n', 'e'), ('s', 'å'), ('ä', 'n'), ('i', 'n'), ('e', 'j'), ('u', 'n'), ('i', 'll'), ('de', 'n'), ('s', 'om'), ('f', 'v'), ('p', 'å'), ('e', 'd'), ('a', 'g'), ('l', 'i'), ('en', 'ne'), ('h', 'enne'), ('i', 'd'), ('r', 'a'), ('h', 'ade'), ('a', 'll'), ('i', 'ng'), ('t', 'a')]\n"
     ]
    }
   ],
   "source": [
    "print(merge_ops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use the vocabulary you obtained to tokenize a text stored in the corpus string.\n",
    "\n",
    "You will apply the merge operations in the same order you created them. You will call this function `tokenize_bpe()` that will take two inputs: `corpus` and `merge_ops`, and that will return the tokenized text in the form of a list.\n",
    "\n",
    "    def tokenize_bpe(corpus, merge_ops):\n",
    "\n",
    "      ...\n",
    "\n",
    "      return tokens\n",
    "\n",
    "This function is easy. Just reuse the `merge_bigrams()` function and a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def tokenize_bpe(corpus, merge_ops):\n",
    "    corpus_l = list(corpus)\n",
    "    ...\n",
    "    return corpus_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'e', 'l', 'm', 'a', ' ', 'L', 'ag', 'er', 'l', 'ö', 'f', ' ', 'E', 'n', ' ', 'h', 'er', 'r', 'g', 'å', 'r', 'd', 's', 's', 'ä', 'g', 'en', ' ', 'B', 'o', 'k', 'u', 't', 'g', 'å', 'v', 'a', ' ', 'A', 'l', 'b', 'er', 't', ' ', 'B', 'on', 'n', 'i', 'er', 's', ' ', 'för', 'l', 'ag', ',', ' ', 'S', 't', 'o', 'ck', 'h', 'o', 'l', 'm', ' ', '1', '8', '9', '9', '.', ' ', 'I', '.', ' ', 'D', 'et', ' ', 'var', ' ', 'en', ' ', 'sk', 'ö', 'n', ' ', 'h', 'ö', 'st', 'd', 'ag', ' ', 'h', 'ä', 'ne', 'm', 'o', 't', ' ', 's', 'l', 'u', 't', 'et', ' ', 'a', 'f', ' ', 't', 'r', 'e', 'tt', 'i', 'o', 'ta', 'l', 'et', '.', ' ', 'P', 'å', ' ', 'den', ' ', 't', 'i', 'den', ' ', 'f', 'an', 'n', 's', ' ', 'i', ' ', 'U', 'p', 's', 'a', 'l', 'a', ' ', 'e', 'tt', ' ', 'h', 'ö', 'g', 't', ',', ' ', 'g', 'u', 'l', 't', ' ', 't', 'v', 'å', 'v', 'å', 'n', 'ing', 's', 'h', 'u', 's', ',', ' ', 'som', ' ', 'st', 'o', 'd', ' ', 'un', 'de', 'r', 'l', 'ig', 't', ' ', 'en', 's', 'a', 'm', 't', ' ', 'på', ' ', 'en', ' ', 'li', 't', 'en', ' ', 'ä', 'ng', ',', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_bpe(corpus, merge_ops)[:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now done with BPE and you can now consider the unigram language model.\n",
    "\n",
    "Read these two sections:\n",
    "\n",
    "1. Section 3.2 of _Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates_ (https://arxiv.org/pdf/1804.10959.pdf) by Kudo (2018).\n",
    "2. Section 2, algorithm 2 and the related text of _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ (https://aclanthology.org/2020.findings-emnlp.414.pdf) by Bostrom and Durrett (2020).\n",
    "\n",
    "In your report, **summarize** (10 to 15 lines or so) with your own words the tokenization with a unigram language model as described by by Kudo (2018) and Bostrom and Durrett (2020). You will notably consider two aspects:\n",
    "1. How to obtain the subword vocabulary;\n",
    "2. How to tokenize a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your report, given what you have done on the byte-pair encoding, how would you build the “reasonably big seed vocabulary” needed for the unigram language model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the “reasonably big seed vocabulary”, you will now fit a unigram language model. You will start with a vocabulary of 50 subwords in addition to the character set and reduce it to 49, i.e. you will find one subword to discard.\n",
    "\n",
    "Kudo (2018) proposes the expectation-maximization algorithm. We will ignore this step. Instead, in this lab, you will approximate the language model with the BPE algorithm.\n",
    "\n",
    "Write a `unigram_lm()` function that takes a corpus string and a vocabulary of subword tokens as input and returns a dictionary, where the keys are the subwords and each key value, the key relative frequency:\n",
    "\n",
    "    def unigram_lm(corpus, vocabulary):\n",
    "\n",
    "       ...\n",
    "\n",
    "      return unigram_probs\n",
    "Your function will:\n",
    "\n",
    "1. Tokenize your corpus with BPE (you can reuse the `tokenize_bpe()` function);\n",
    "2. Estimate the probability of each word (simply count the occurrences of the subwords and divide them by the length of the tokenized corpus);\n",
    "3. Return this model as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def unigram_lm(corpus, merge_ops):\n",
    "    tokenized_corpus = tokenize_bpe(corpus, merge_ops)\n",
    "    unigram_probs = {}\n",
    "    cnt = 0\n",
    "    ...\n",
    "    return unigram_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': 0.0017194185598387304,\n",
       " 'e': 0.03049497514748461,\n",
       " 'l': 0.03290611381760329,\n",
       " 'm': 0.028795320019368163,\n",
       " 'a': 0.040515035030682725,\n",
       " 'L': 0.0003755052027234009,\n",
       " 'ag': 0.004713578465764795,\n",
       " 'er': 0.010830360583811774,\n",
       " 'ö': 0.010069468462503828,\n",
       " 'f': 0.021018409636649308,\n",
       " 'E': 0.0002668063282508375,\n",
       " 'n': 0.017569690801110704,\n",
       " 'h': 0.015751455082660553,\n",
       " 'r': 0.030929770645374863,\n",
       " 'g': 0.024684526221133037,\n",
       " 'å': 0.021225925669733293,\n",
       " 'd': 0.029457394982064684,\n",
       " 's': 0.03798531577023034,\n",
       " 'ä': 0.015257369289603447,\n",
       " 'en': 0.02391375238396395,\n",
       " 'B': 0.0008103007006136546,\n",
       " 'o': 0.015810745377827406,\n",
       " 'k': 0.024486891903910193,\n",
       " 'u': 0.02201646293862466,\n",
       " 't': 0.0441910333310276,\n",
       " 'v': 0.014605176042768066,\n",
       " 'A': 0.0006225480992519541,\n",
       " 'b': 0.015079498404102889,\n",
       " 'on': 0.00571163176774015,\n",
       " 'i': 0.020682431297370477,\n",
       " 'för': 0.009061533444667333,\n",
       " ',': 0.027095664891251717,\n",
       " 'ck': 0.00784608239374685,\n",
       " '1': 9.88171586114213e-06,\n",
       " '8': 9.88171586114213e-06,\n",
       " '9': 1.976343172228426e-05,\n",
       " '.': 0.022125161813097226,\n",
       " 'I': 0.0031127404962597704,\n",
       " 'D': 0.005484352302933882,\n",
       " 'et': 0.009170232319139895,\n",
       " 'var': 0.010128758757670681,\n",
       " 'sk': 0.008251232744053677,\n",
       " 'st': 0.016126960285383955,\n",
       " 'ne': 0.001462493947449035,\n",
       " 'tt': 0.008656383094360504,\n",
       " 'ta': 0.0037155251637894402,\n",
       " 'P': 0.0002371611806674111,\n",
       " 'den': 0.0053855351443224606,\n",
       " 'an': 0.016798916963941618,\n",
       " 'U': 0.00025692461238969533,\n",
       " 'p': 0.012075456782315681,\n",
       " 'ing': 0.003735288595511725,\n",
       " 'som': 0.005355889996739034,\n",
       " 'un': 0.005533760882239592,\n",
       " 'de': 0.016126960285383955,\n",
       " 'ig': 0.011255274365840885,\n",
       " 'på': 0.005158255679516191,\n",
       " 'li': 0.004604879591292232,\n",
       " 'ng': 0.006996254829688627,\n",
       " 'or': 0.007302588021384033,\n",
       " 'ade': 0.007954781268219413,\n",
       " 'och': 0.01072166170933921,\n",
       " 'det': 0.006205717560797257,\n",
       " 'in': 0.005761040347045861,\n",
       " 'x': 0.0004051503503068273,\n",
       " 'är': 0.008093125290275404,\n",
       " 'id': 0.0043973635582082475,\n",
       " 'så': 0.005879620937379567,\n",
       " 'att': 0.012865994051207051,\n",
       " 'all': 0.003982331492040278,\n",
       " 'om': 0.010553672539699794,\n",
       " 'ra': 0.004377600126485963,\n",
       " 'fv': 0.005316363133294465,\n",
       " 'H': 0.0074310503275788805,\n",
       " 'ar': 0.009338221488779311,\n",
       " 'ed': 0.004772868760931648,\n",
       " 'han': 0.007322351453106318,\n",
       " 'ill': 0.005434943723628171,\n",
       " 'än': 0.005800567210490429,\n",
       " 'hade': 0.004011976639623704,\n",
       " 'y': 0.007233416010356038,\n",
       " 'll': 0.005909266084962993,\n",
       " 'hon': 0.010099113610087255,\n",
       " 'na': 0.00686779252349378,\n",
       " '–': 0.0054942340187950234,\n",
       " 'j': 0.009693963259780428,\n",
       " '?': 0.0011759241874759133,\n",
       " 'Å': 0.0004545589296125379,\n",
       " 'J': 0.0011759241874759133,\n",
       " 'ej': 0.005701750051879008,\n",
       " ':': 0.00011858059033370555,\n",
       " 'G': 0.0006719566785576647,\n",
       " 'F': 0.0005632578040851013,\n",
       " 'ör': 0.0039526863444568515,\n",
       " 'M': 0.0024506655335632477,\n",
       " 'T': 0.00029645147583426385,\n",
       " 'V': 0.0005237309406405328,\n",
       " '!': 0.0006225480992519541,\n",
       " 'O': 0.0017095368439775883,\n",
       " 'ch': 0.0014723756633101771,\n",
       " 'enne': 0.00011858059033370555,\n",
       " '»': 4.9408579305710646e-05,\n",
       " 'N': 0.0006521932468353805,\n",
       " 'henne': 0.0044467721375139576,\n",
       " 'K': 0.0003458600551399745,\n",
       " 'c': 7.905372688913703e-05,\n",
       " 'C': 2.9645147583426388e-05,\n",
       " 'z': 1.976343172228426e-05,\n",
       " 'Ä': 0.00011858059033370555,\n",
       " ';': 2.9645147583426388e-05,\n",
       " '_': 0.0013439133571153295,\n",
       " 'Ö': 6.91720110279949e-05,\n",
       " 'R': 0.00014822573791713192,\n",
       " 'é': 9.881715861142129e-05,\n",
       " '’': 9.88171586114213e-06,\n",
       " '-': 9.88171586114213e-06,\n",
       " 'X': 1.976343172228426e-05}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_probs = unigram_lm(corpus, merge_ops)\n",
    "unigram_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now apply your unigram language model to tokenize a character sequence that does not include spaces, typically a single word in the Latin or Greek scripts or a sequence of words in Asian scripts, like Chinese or Korean.\n",
    "\n",
    "Write a `tokenize_lm()` function that takes a character sequence, `char_seq`, and a dictionary of unigram probabilities, `unigram_probs`,  as input and returns the subword tokens and the segmentation probability, (prob,tokens). You will only return the token list with the highest probability.\n",
    "\n",
    "    def tokenize_lm(char_seq, unigram_probs):\n",
    "\n",
    "      ...\n",
    "\n",
    "      return max(candidates)\n",
    "\n",
    "As an example, applying \n",
    "\n",
    "`tokenize_lm('senare', unigram_probs)`\n",
    "results in\n",
    "\n",
    "`(2.0899522820189735e-07, ['s', 'en', 'ar', 'e'])`\n",
    "\n",
    "Your function will cache (memoize) the results to speed up the computation. It will be similar to that of Norvig's in the notebook: How to Do Things with Words.ipynb. You can reuse it.\n",
    "Python has a built-in memoization function that you can use: @functools.lru_cache(maxsize=2**10). You can also use the newer @functools.cache() function if you have Python 3.9 or higher. See here: https://docs.python.org/3/library/functools.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "def tokenize_lm(char_seq, unigram_probs):\n",
    "    # Use one of the two cache functions below to have a faster answer:\n",
    "    # @functools.lru_cache(maxsize=2**10)\n",
    "    @functools.cache  # Available from Python 3.9\n",
    "    # The arguments of the cached function must be hashable that's why we define an inner cacheable function\n",
    "    def __tokenize_lm(char_seq):\n",
    "        # Write your code here\n",
    "        ...\n",
    "\n",
    "    return __tokenize_lm(char_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.5867587292712403e-07, ['s', 'en', 'ar', 'e'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_lm('senare', unigram_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tokenization with Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous function applies to a sequence without spaces. You will now apply it to your corpus. Write a `tokenize_text_lm()` function that takes the whole `corpus` string as input and the unigram probabilities `unigram_probs` and return the corpus probability and the tokenized subwords. \n",
    "\n",
    "This function is just an application of the functions you just wrote, where you will:\n",
    "1. `str.split()` the string by whitespaces\n",
    "2. Break the tokens into subtokens using `tokenize_lm()`. This function will return the probabilities of the resulting sequences;\n",
    "3. Sum the logarithm of these probabilities. Use log10 to check your output with the numbers in the notebook. \n",
    "\n",
    "It is very significant that you use the logarithm of the probabilities and the sum. If you multiply the probabilities, you will get an underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def tokenize_text_lm(corpus, unigram_probs):\n",
    "    tokenized_corpus = []\n",
    "    corpus_prob = 0.0\n",
    "    ...\n",
    "    return corpus_prob, tokenized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_loglikelihood, tokens = tokenize_text_lm(corpus, unigram_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-183394.0764900353, ['▁S', 'e', 'l', 'm', 'a', '▁L', 'ag', 'er', 'l', 'ö'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_loglikelihood, tokens[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement the final loop, where you will, at each iteration:\n",
    "1. Select one subword and remove it from your vocabulary.\n",
    "2. Estimate the probabilities of the subwords of the reduced vocabulary using `unigram_lm()`\n",
    "2. Compute the resulting log-likelihood of the corpus without this word. You will use `tokenize_text_lm()` this time\n",
    "3. Compute the loss, i.e. the log-likelihood reduction when the subword is removed from the current vocabulary\n",
    "\n",
    "You will always keep the single characters in your vocabulary to avoid unknown words.\n",
    "\n",
    "Store the pairs, (log-likelihood, removed_subword) in a list `logloss_word` and rank them by likelihood value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_word = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'a', 's', 'l', 'r', 'e', 'd', 'm', ',', 'g']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To have the same results\n",
    "vocabulary_sorted = sorted(vocabulary, key=lambda w: -unigram_probs[w])\n",
    "vocabulary_sorted[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/117 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [05:25<00:00,  2.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "for i, word in enumerate(tqdm.tqdm(vocabulary_sorted)):\n",
    "    if len(word) == 1:\n",
    "        continue\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-10524.100611375936, 'ta'),\n",
       " (-10635.548926578631, 'ra'),\n",
       " (-10660.05402475709, 'ag'),\n",
       " (-10673.84841250212, 'var'),\n",
       " (-10673.84841250212, 'som'),\n",
       " (-10673.84841250212, 'och'),\n",
       " (-10673.84841250212, 'ing'),\n",
       " (-10673.84841250212, 'ill'),\n",
       " (-10673.84841250212, 'hon'),\n",
       " (-10673.84841250212, 'henne'),\n",
       " (-10673.84841250212, 'han'),\n",
       " (-10673.84841250212, 'hade'),\n",
       " (-10673.84841250212, 'för'),\n",
       " (-10673.84841250212, 'enne'),\n",
       " (-10673.84841250212, 'det'),\n",
       " (-10673.84841250212, 'den'),\n",
       " (-10673.84841250212, 'att'),\n",
       " (-10673.84841250212, 'all'),\n",
       " (-10673.84841250212, 'ade'),\n",
       " (-10779.939897996781, 'id'),\n",
       " (-10786.685836769728, 'ed'),\n",
       " (-10801.597924158617, 'li'),\n",
       " (-10817.553130551387, 'ng'),\n",
       " (-10869.284547945834, 'na'),\n",
       " (-10873.334727177222, 'så'),\n",
       " (-10905.971702064562, 'fv'),\n",
       " (-10918.84405772877, 'et'),\n",
       " (-10976.091295432212, 'ne'),\n",
       " (-10985.80312557827, 'in'),\n",
       " (-11004.788105880085, 'or'),\n",
       " (-11007.224169128225, 'sk'),\n",
       " (-11033.673005600547, 'tt'),\n",
       " (-11053.38942833431, 'un'),\n",
       " (-11084.136206143157, 'på'),\n",
       " (-11131.730916103232, 'ig'),\n",
       " (-11132.659313287644, 'ej'),\n",
       " (-11163.493759251316, 'än'),\n",
       " (-11202.206326932559, 'st'),\n",
       " (-11230.503947208956, 'är'),\n",
       " (-11231.200537914934, 'er'),\n",
       " (-11363.683710929297, 'ar'),\n",
       " (-11373.377828648227, 'll'),\n",
       " (-11834.194580386888, 'om'),\n",
       " (-11888.84333135726, 'ör'),\n",
       " (-11921.409049764305, 'ck'),\n",
       " (-12088.997689454845, 'on'),\n",
       " (-12149.157992678462, 'an'),\n",
       " (-12344.774840508442, 'ch'),\n",
       " (-12773.337788262084, 'de'),\n",
       " (-13257.468421443453, 'en')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(logloss_word, reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will reduce now your vocabulary by one token: `out_candidate`. Write the piece of code to determine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "out_candidate = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ta'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_candidate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tokenization without the subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_sorted.remove('ta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_probs = unigram_lm(corpus, vocabulary_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loglikelihood, tokens = tokenize_text_lm(corpus, unigram_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-183394.0764900353"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-193918.17710141125"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loglikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁S', 'e', 'l', 'm', 'a', '▁L', 'ag', 'er', 'l', 'ö']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, you can improve this program and test it on larger corpora. You can also read a fine implementation of BPE by Andrej Karpathy: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will write a report where you will:\n",
    "1. Describe the background as well as the algorithms you used. For this, summarize the articles as described in the notebook:\n",
    "   * Preliminaries: subword tokenizers\n",
    "   * Design of the BPE Algorithm\n",
    "   * Unigram Language Model\n",
    "2. Describe your program as well as your results\n",
    "\n",
    "The whole report should be of 2 to 3 pages.\n",
    "\n",
    "Submit your report as well as your **notebook** (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, you can either\n",
    "1. Write directly your text in Canvas, or\n",
    "2. Use Latex and Overleaf (www.overleaf.com). This will probably help you structure your text. You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is September 29, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curious?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, you can read the sentencepiece implementation by the author:\n",
    "https://github.com/google/sentencepiece/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning BPE, you can also read a fine implementation of BPE by Andrej Karpathy: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
