{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Classifier on the *Salammb√¥* Dataset with PyTorch\n",
    "Author: Pierre Nugues\n",
    "\n",
    "We use three classes: French, English, and German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "We can read the data from a file with the svmlight format or directly create numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "    [[35680, 2217], [42514, 2761], [15162, 990], [35298, 2274],\n",
    "     [29800, 1865], [40255, 2606], [74532, 4805], [37464, 2396],\n",
    "     [31030, 1993], [24843, 1627], [36172, 2375], [39552, 2560],\n",
    "     [72545, 4597], [75352, 4871], [18031, 1119], [36961, 2503],\n",
    "     [43621, 2992], [15694, 1042], [36231, 2487], [29945, 2014],\n",
    "     [40588, 2805], [75255, 5062], [37709, 2643], [30899, 2126],\n",
    "     [25486, 1784], [37497, 2641], [40398, 2766], [74105, 5047],\n",
    "     [76725, 5312], [18317, 1215]\n",
    "     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add German data and we adjust `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_de = np.array(\n",
    "    [[37599, 1771], [44565, 2116], [16156, 715], [37697, 1804],\n",
    "     [29800, 1865], [42606, 2146], [78242, 3813], [40341, 1955],\n",
    "     [31030, 1993], [26676, 1346], [39250, 1902], [41780, 2106],\n",
    "     [72545, 4597], [79195, 3988], [19020, 928]\n",
    "     ])\n",
    "\n",
    "X = np.vstack((X, X_de))\n",
    "\n",
    "y = np.array(\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "     2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40, 19, 34, 12,  3, 29, 10, 20, 35, 41,  2, 13, 31, 39, 18, 26, 25,\n",
       "       38, 23, 44, 30, 32, 36,  5, 24, 33, 43,  9, 16, 27, 28,  6, 14,  7,\n",
       "        1, 11,  0,  8, 37, 21, 22, 15, 42,  4, 17])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(45)\n",
    "np.random.shuffle(indices)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 0, 0, 1, 0, 1, 2, 2, 0, 0, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2,\n",
       "       2, 0, 1, 2, 2, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 2, 0,\n",
       "       1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X[indices, :]\n",
    "y = y[indices]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data\n",
    "Scaling and normalizing are usually very significant with neural networks. We use sklean transformers. They consist of two main methods: `fit()` and `transform()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99882795, 0.0484018 ],\n",
       "       [0.99774592, 0.06710504],\n",
       "       [0.99804736, 0.06246169],\n",
       "       [0.9979983 , 0.06324072]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "X_norm = normalizer.fit_transform(X)\n",
    "X_norm[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.58883716, -1.60257285],\n",
       "       [-0.73955946,  0.72879853],\n",
       "       [-0.09088476,  0.15000209],\n",
       "       [-0.19645636,  0.24710915]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_scaled = scaler.fit_transform(X_norm)\n",
    "X_scaled[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 2, 0, 0, 1, 0, 1, 2, 2, 0, 0, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 0,\n",
       "        1, 2, 2, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 2, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = torch.Tensor(X_scaled)\n",
    "y = torch.LongTensor(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a seed to have reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a classifier equivalent to a logistic regression. With PyTorch, the crossentropy loss computes the softmax of the outputs. We do not add an activation in the last layer. \n",
    "\n",
    "The outputs are then called, rather improperly, logits. For a clarification on this terrible terminology, see here: https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow/52111173#52111173 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 10)\n",
    "        self.fc2 = nn.Linear(10, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try the network with one hidden layer, set `complex` to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_scaled.shape[1]\n",
    "if not complex:\n",
    "    model = Model(input_dim)\n",
    "else:\n",
    "    model = Model2(input_dim)\n",
    "loss_fn = nn.CrossEntropyLoss()    # cross entropy loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(X_scaled, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9317493650648329\n",
      "0.5588877361267806\n",
      "0.44014442722416586\n",
      "0.3636842472996149\n",
      "0.3173683798613234\n",
      "0.28952390769393077\n",
      "0.2709604480277954\n",
      "0.25857085826185844\n",
      "0.2482782617664068\n",
      "0.23932697284294085\n",
      "0.2336660644058914\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    loss_train = 0\n",
    "    for X_scaled_batch, y_batch in dataloader:\n",
    "        y_batch_pred = model(X_scaled_batch)\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(loss_train/len(y))\n",
    "print(loss_train/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-3.5928e-01,  3.3271e-01],\n",
       "                      [ 9.9213e-01, -9.1046e-01],\n",
       "                      [ 1.3038e+00, -6.2517e-02],\n",
       "                      [ 1.0074e-01,  6.1619e-01],\n",
       "                      [ 1.4695e+00, -3.0734e-01],\n",
       "                      [ 4.3213e-01, -7.4602e-01],\n",
       "                      [-1.1247e+00,  3.5175e-01],\n",
       "                      [-1.3331e+00,  1.0375e+00],\n",
       "                      [ 1.0232e-01,  1.6791e-04],\n",
       "                      [-1.0063e+00,  3.6245e-01]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.2754,  0.0943,  1.2864, -0.5712,  1.6248, -0.4456,  0.4527,  0.0055,\n",
       "                      -0.2473,  0.0170])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-2.2804e-01, -7.0409e-01,  8.0370e-01,  2.7427e-01,  7.7009e-01,\n",
       "                       -4.1772e-01, -1.0822e-01, -3.2071e-01, -1.5380e-03, -7.0085e-02],\n",
       "                      [ 4.9562e-01, -2.2085e-01, -1.2069e+00, -2.0575e-01, -1.5458e+00,\n",
       "                       -2.4551e-01,  5.9181e-01,  1.2051e+00, -9.8393e-02,  9.2855e-01],\n",
       "                      [ 1.4884e-01,  8.5398e-01,  7.3222e-01, -3.0372e-01,  1.0009e+00,\n",
       "                        2.7169e-01, -6.4313e-01, -9.4443e-01, -2.8529e-01, -3.9486e-01]])),\n",
       "             ('fc2.bias', tensor([ 0.5192, -0.8866, -0.2524]))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "### Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the probabilities to belong to the classes for all the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model2(\n",
       "  (fc1): Linear(in_features=2, out_features=10, bias=True)\n",
       "  (fc2): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output with no activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  3.9234, -12.9865,   9.7991],\n",
       "        [  0.1311,   2.3797,  -2.7473],\n",
       "        [  2.3955,  -3.6677,   1.3206],\n",
       "        [  2.0336,  -2.6955,   0.6480]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_logits = model(X_scaled)\n",
    "Y_pred_logits[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7991e-03, 1.2680e-10, 9.9720e-01],\n",
       "        [9.4966e-02, 8.9969e-01, 5.3394e-03],\n",
       "        [7.4423e-01, 1.7318e-03, 2.5404e-01],\n",
       "        [7.9428e-01, 7.0174e-03, 1.9870e-01]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_proba = F.softmax(model(X_scaled), dim=-1)\n",
    "Y_pred_proba[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 2, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recompute it with matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_params = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7991e-03, 1.2680e-10, 9.9720e-01],\n",
      "        [9.4966e-02, 8.9969e-01, 5.3394e-03],\n",
      "        [7.4423e-01, 1.7318e-03, 2.5404e-01],\n",
      "        [7.9428e-01, 7.0174e-03, 1.9870e-01]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if complex:\n",
    "    print(torch.softmax(torch.relu(X_scaled @ m_params[0].T + m_params[1]) @ m_params[2].T + m_params[3], dim=-1)[:4])\n",
    "else:\n",
    "    print(torch.softmax(X_scaled @ m_params[0].T + m_params[1], dim=-1)[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0, 0, 0, 1, 0, 1, 2, 2, 0, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 2, 2, 0,\n",
       "        1, 2, 2, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = torch.argmax(Y_pred_proba, dim=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "We recompute the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0028, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(Y_pred_logits[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0028, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(Y_pred_proba[0])[y[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2286, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(Y_pred_logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2286, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.mean(torch.log(Y_pred_proba[range(0, len(y)), y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        15\n",
      "           1       1.00      1.00      1.00        15\n",
      "           2       1.00      0.80      0.89        15\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.94      0.93      0.93        45\n",
      "weighted avg       0.94      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We computed the accuracy from the training set. This is not a good practice. We should use a dedicated test set instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
